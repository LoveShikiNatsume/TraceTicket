# -*- coding: utf-8 -*-
"""
Train Ticket å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿä¸»æ§åˆ¶å™¨

Author: LoveShikiNatsume
Date: 2025-07-01
Version: 1.6 ä¿®æ”¹å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼ŒåŸºäºCSVæ ‡ç­¾è¿›è¡Œå‡†ç¡®æ€§éªŒè¯
"""

import os
import sys
import time
import json
import logging
import subprocess
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path
import requests  # æ·»åŠ è¿™è¡Œ

# æ·»åŠ å­ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "train-ticket-trace-collect"))

class TrainTicketAnomalyDetectionController:
    """Train Ticket å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.config = self._load_default_config()
        self.logger = self._setup_logging()
        
        # è®°å½•è„šæœ¬å¯åŠ¨æ—¶é—´ï¼Œåªå¤„ç†å¯åŠ¨åçš„æ•°æ®
        self.script_start_time = datetime.now()
        
        # å„ç»„ä»¶çš„çŠ¶æ€
        self.component_status = {
            "load_test": "æœªå¼€å§‹",
            "data_collection": "æœªå¼€å§‹",
            "metrics_collection": "æœªå¼€å§‹",
            "anomaly_detection": "æœªå¼€å§‹",
            "tracevae_service": "æœªå¼€å§‹"  # æ·»åŠ æœåŠ¡çŠ¶æ€
        }
        
        # å‹æµ‹ç»Ÿè®¡
        self.load_test_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "last_update": None
        }
        
        # å‹æµ‹ç»Ÿè®¡
        self.load_test_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "last_update": None
        }
        
        # è¿è¡Œç»“æœå­˜å‚¨
        self.results = {
            "start_time": None,
            "end_time": None,
            "real_time_detections": []
        }
        
        # å®æ—¶ç›‘æ§ç›¸å…³
        self.monitoring_active = False
        self.last_processed_minute = None
        
        # TraceVAEæœåŠ¡ç›¸å…³
        self.tracevae_service_process = None
        self.tracevae_service_url = os.getenv("TRACEVAE_SERVICE_URL", "http://localhost:8000")
        
        self.logger.info("Train Ticket å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿå¯åŠ¨")

    def _load_default_config(self) -> Dict:
        """åŠ è½½é»˜è®¤é…ç½®"""
        return {
            "real_time_mode": {
                "enabled": True,
                "check_interval_seconds": 1,  # æ£€æŸ¥æ–°æ–‡ä»¶çš„é—´éš”ï¼Œæ”¹ä¸º1ç§’
                "status_display_interval_seconds": 30,  # çŠ¶æ€æ˜¾ç¤ºé—´éš”ï¼Œä¿æŒ30ç§’
                "auto_process_delay_seconds": 65,
                "detection_threshold": 0.15,
                "warmup_minutes": 3
            },
            "data_collection": {
                "interval_seconds": 60,
                "lookback_period": "5m"
            },
            "scripts": {
                "anomaly_detection": "anomaly-detection/vae_detector.py"
            }
        }

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('MainController')
        logger.setLevel(logging.INFO)
        
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - [ä¸»æ§åˆ¶å™¨] - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        return logger

    def _run_subprocess(self, cmd: List[str], component_name: str, 
                       timeout: Optional[int] = None, 
                       background: bool = False) -> subprocess.Popen:
        """è¿è¡Œå­è¿›ç¨‹"""
        self.logger.info(f"å¯åŠ¨ç»„ä»¶: {component_name}")
        
        try:
            if background:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    cwd=str(self.project_root)
                )
                self.component_status[component_name.lower().replace(' ', '_')] = "è¿è¡Œä¸­"
                return process
            else:
                result = subprocess.run(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    timeout=timeout,
                    cwd=str(self.project_root)
                )
                
                if result.returncode == 0:
                    self.logger.info(f"{component_name} æ‰§è¡Œå®Œæˆ")
                    self.component_status[component_name.lower().replace(' ', '_')] = "å®Œæˆ"
                else:
                    self.logger.error(f"{component_name} æ‰§è¡Œå¤±è´¥: {result.stderr}")
                    self.component_status[component_name.lower().replace(' ', '_')] = "å¤±è´¥"
                
                return result
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"{component_name} æ‰§è¡Œè¶…æ—¶")
            self.component_status[component_name.lower().replace(' ', '_')] = "è¶…æ—¶"
            return None
        except Exception as e:
            self.logger.error(f"{component_name} æ‰§è¡Œå¼‚å¸¸: {e}")
            self.component_status[component_name.lower().replace(' ', '_')] = "å¼‚å¸¸"
            return None

    def start_load_test(self) -> Optional[subprocess.Popen]:
        """å¯åŠ¨å‹æµ‹è„šæœ¬"""
        self.logger.info("å¯åŠ¨å‹æµ‹æ¨¡å—...")
        
        run_script = self.project_root / "train-ticket-auto-query" / "run.py"
        
        if not os.path.exists(run_script):
            self.logger.error(f"å‹æµ‹è„šæœ¬ä¸å­˜åœ¨: {run_script}")
            return None
        
        try:
            cmd = [sys.executable, str(run_script)]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡å¯ç”¨å®‰é™æ¨¡å¼
            env = os.environ.copy()
            env['PRESSURE_TEST_QUIET'] = 'true'
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                universal_newlines=True,
                bufsize=1,
                cwd=str(self.project_root),
                env=env
            )
            
            # å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥è¯»å–å‹æµ‹è¾“å‡ºå¹¶ç»Ÿè®¡æˆåŠŸè¯·æ±‚æ•°
            def read_and_count_stats():
                try:
                    for line in process.stdout:
                        line = line.strip()
                        if "[å‹æµ‹çŠ¶æ€]" in line and "å®Œæˆè¯·æ±‚:" in line and "æˆåŠŸç‡:" in line:
                            try:
                                parts = line.split("å®Œæˆè¯·æ±‚:")[1].split(",")[0].strip()
                                total_requests = int(parts)
                                
                                success_parts = line.split("æˆåŠŸç‡:")[1].split("%")[0].strip()
                                success_rate = float(success_parts)
                                
                                successful_requests = int(total_requests * success_rate / 100)
                                
                                self.load_test_stats["total_requests"] = total_requests
                                self.load_test_stats["successful_requests"] = successful_requests
                                self.load_test_stats["last_update"] = datetime.now()
                                
                            except (ValueError, IndexError):
                                pass
                except:
                    pass
            
            stats_thread = threading.Thread(target=read_and_count_stats, daemon=True)
            stats_thread.start()
            
            time.sleep(3)
            if process.poll() is None:
                self.logger.info("å‹æµ‹æ¨¡å—å¯åŠ¨æˆåŠŸ")
                self.component_status["load_test"] = "è¿è¡Œä¸­"
                return process
            else:
                self.logger.error(f"å‹æµ‹å¯åŠ¨å¤±è´¥ï¼Œé€€å‡ºç : {process.returncode}")
                return None
                
        except Exception as e:
            self.logger.error(f"å‹æµ‹å¯åŠ¨å¼‚å¸¸: {e}")
            return None

    def start_data_collection(self, duration_minutes: int = 0) -> Optional[subprocess.Popen]:
        """å¯åŠ¨æ•°æ®é‡‡é›†"""
        collector_script = self.project_root / "train-ticket-trace-collect" / "trace_collector.py"
        
        if not collector_script.exists():
            self.logger.error(f"æ•°æ®é‡‡é›†è„šæœ¬ä¸å­˜åœ¨: {collector_script}")
            return None
        
        try:
            cmd = [
                sys.executable, str(collector_script),
                "--duration", str(duration_minutes)
            ]
            
            process = self._run_subprocess(cmd, "data_collection", background=True)
            if process:
                self.logger.info("æ•°æ®é‡‡é›†å¯åŠ¨æˆåŠŸ")
                return process
            else:
                self.logger.error("æ•°æ®é‡‡é›†å¯åŠ¨å¤±è´¥")
                return None
        except Exception as e:
            self.logger.error(f"æ•°æ®é‡‡é›†å¯åŠ¨å¼‚å¸¸: {e}")
            return None

    def start_metrics_collection(self, duration_minutes: int = 0) -> Optional[subprocess.Popen]:
        """å¯åŠ¨æŒ‡æ ‡é‡‡é›†"""
        metrics_script = self.project_root / "train-ticket-trace-collect" / "metrics_collector.py"
        
        if not metrics_script.exists():
            self.logger.warning(f"æŒ‡æ ‡é‡‡é›†è„šæœ¬ä¸å­˜åœ¨: {metrics_script}")
            return None
        
        try:
            cmd = [
                sys.executable, str(metrics_script),
                "--duration", str(duration_minutes),
                "--interval", "1"  # æ·»åŠ 1ç§’é—´éš”å‚æ•°
            ]
            
            process = self._run_subprocess(cmd, "metrics_collection", background=True)
            if process:
                self.logger.info("æŒ‡æ ‡é‡‡é›†å¯åŠ¨æˆåŠŸ")
                return process
            else:
                self.logger.error("æŒ‡æ ‡é‡‡é›†å¯åŠ¨å¤±è´¥")
                return None
        except Exception as e:
            self.logger.error(f"æŒ‡æ ‡é‡‡é›†å¯åŠ¨å¼‚å¸¸: {e}")
            return None

    def check_for_new_data(self, target_date: str = None) -> List[str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„åˆ†é’Ÿçº§æ•°æ®æ–‡ä»¶"""
        target_date = target_date or datetime.now().strftime("%Y-%m-%d")
        trace_dir = self.project_root / "trace" / target_date / "csv"
        
        if not trace_dir.exists():
            return []
        
        csv_files = list(trace_dir.glob("*.csv"))
        new_files = []
        
        current_time = datetime.now()
        
        for csv_file in csv_files:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¯¹åº”çš„.label_processedæ ‡å¿—æ–‡ä»¶
            flag_file = str(csv_file).replace('.csv', '.label_processed')
            
            if os.path.exists(flag_file):
                continue
            
            # è§£ææ–‡ä»¶å
            try:
                filename = csv_file.stem
                hour, minute = filename.split('_')
                
                # æ„é€ æ–‡ä»¶å¯¹åº”çš„æ—¶é—´
                file_time = current_time.replace(hour=int(hour), minute=int(minute), second=0, microsecond=0)
                
                # ä¿®æ”¹æ—¶é—´è¿‡æ»¤é€»è¾‘ï¼šåªè·³è¿‡æ—©äºå¯åŠ¨åˆ†é’Ÿçš„æ–‡ä»¶
                # å°†å¯åŠ¨æ—¶é—´å‘ä¸‹å–æ•´åˆ°åˆ†é’Ÿçº§åˆ«ï¼ŒåŒ…å«å¯åŠ¨å½“åˆ†é’Ÿçš„æ–‡ä»¶
                script_start_minute = self.script_start_time.replace(second=0, microsecond=0)
                
                if file_time < script_start_minute:
                    self.logger.debug(f"è·³è¿‡å†å²æ•°æ®æ–‡ä»¶: {filename} (æ–‡ä»¶æ—¶é—´: {file_time.strftime('%H:%M')}, å¯åŠ¨åˆ†é’Ÿ: {script_start_minute.strftime('%H:%M')})")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»"æˆç†Ÿ"ï¼ˆè¶…è¿‡65ç§’ï¼‰
                time_diff = (current_time - file_time).total_seconds()
                
                if time_diff >= self.config["real_time_mode"]["auto_process_delay_seconds"]:
                    new_files.append(str(csv_file))
                    self.logger.debug(f"æ–°æ•°æ®æ–‡ä»¶: {filename} (ç­‰å¾…æ—¶é—´: {time_diff:.0f}s)")

            except ValueError:
                continue
        
        return new_files

    def process_collected_data(self, csv_file_path: str = None, target_date: str = None):
        """å¤„ç†é‡‡é›†çš„æ•°æ®ï¼ˆå¼‚å¸¸æ ‡ç­¾ç”Ÿæˆï¼‰"""
        processor_script = self.project_root / "train-ticket-trace-collect" / "trace_label_processor.py"
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not processor_script.exists():
            self.logger.error(f"æ ‡ç­¾å¤„ç†è„šæœ¬ä¸å­˜åœ¨: {processor_script}")
            return False
        
        try:
            if csv_file_path:
                # å¤„ç†ç‰¹å®šæ–‡ä»¶
                cmd = [sys.executable, str(processor_script), "--file", csv_file_path]
            else:
                # å¤„ç†æ•´ä¸ªæ—¥æœŸ
                cmd = [sys.executable, str(processor_script)]
                if target_date:
                    cmd.extend(["--date", target_date])
            
            result = self._run_subprocess(cmd, "label_processing", timeout=300)
            return result is not None and (not hasattr(result, 'returncode') or result.returncode == 0)
        except Exception as e:
            self.logger.error(f"æ ‡ç­¾å¤„ç†å¼‚å¸¸: {e}")
            return False

    def run_real_time_monitoring(self):
        """è¿è¡Œå®æ—¶ç›‘æ§æ¨¡å¼"""
        self.logger.info("å¯åŠ¨å®æ—¶å¼‚å¸¸æ£€æµ‹ç›‘æ§ç³»ç»Ÿ")
        self.logger.info("ç³»ç»Ÿé…ç½®:")
        self.logger.info(f"  - æ£€æµ‹é˜ˆå€¼: {self.config['real_time_mode']['detection_threshold']}")
        self.logger.info(f"  - æ£€æŸ¥é—´éš”: {self.config['real_time_mode']['check_interval_seconds']} ç§’")
        self.logger.info(f"  - çŠ¶æ€æ˜¾ç¤ºé—´éš”: {self.config['real_time_mode']['status_display_interval_seconds']} ç§’")
        self.logger.info(f"è„šæœ¬å¯åŠ¨æ—¶é—´: {self.script_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.monitoring_active = True
        self.results["start_time"] = datetime.now().isoformat()
        
        # å¯åŠ¨å‹æµ‹
        load_test_process = self.start_load_test()
        if not load_test_process:
            self.logger.warning("å‹æµ‹å¯åŠ¨å¤±è´¥ï¼Œç»§ç»­ç›‘æ§")
        
        # å¯åŠ¨æ•°æ®é‡‡é›†
        collection_process = self.start_data_collection(duration_minutes=0)
        
        # å¯åŠ¨æŒ‡æ ‡é‡‡é›†
        metrics_process = self.start_metrics_collection(duration_minutes=0)
        if not metrics_process:
            self.logger.warning("æŒ‡æ ‡é‡‡é›†å¯åŠ¨å¤±è´¥ï¼Œä½†å°†ç»§ç»­ç›‘æ§")
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦æˆåŠŸå¯åŠ¨
        if not collection_process:
            self.logger.error("æ•°æ®é‡‡é›†å¯åŠ¨å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç›‘æ§")
            self.monitoring_active = False
            return
        
        # å¯åŠ¨ç›‘æ§å¾ªç¯
        check_interval = self.config["real_time_mode"]["check_interval_seconds"]
        status_display_interval = self.config["real_time_mode"]["status_display_interval_seconds"]
        self.logger.info(f"å¼€å§‹å®æ—¶ç›‘æ§ (æ£€æŸ¥é—´éš”: {check_interval}s, çŠ¶æ€æ˜¾ç¤ºé—´éš”: {status_display_interval}s)")
        
        start_time = time.time()
        last_status_display = 0  # è®°å½•ä¸Šæ¬¡çŠ¶æ€æ˜¾ç¤ºæ—¶é—´
        
        try:
            while self.monitoring_active:
                current_time = time.time()
                elapsed_minutes = (current_time - start_time) / 60
                
                # æ£€æŸ¥æ–°æ•°æ®
                new_files = self.check_for_new_data()
                
                if new_files:
                    self.process_new_files_real_time(new_files)
                
                # åªåœ¨è¾¾åˆ°çŠ¶æ€æ˜¾ç¤ºé—´éš”æ—¶æ‰æ˜¾ç¤ºçŠ¶æ€
                if current_time - last_status_display >= status_display_interval:
                    elapsed_hours = elapsed_minutes / 60
                    detection_count = len(self.results["real_time_detections"])
                    anomaly_count = len([d for d in self.results["real_time_detections"] if d["result"].get("anomaly_detected", False)])
                    
                    # è®¡ç®—éªŒè¯ç»Ÿè®¡
                    all_detections = self.results["real_time_detections"]
                    validated_detections = [d for d in all_detections if d["validation"].get("validation") == "completed"]
                    
                    accuracy_stats = {"true_positive": 0, "true_negative": 0, "false_positive": 0, "false_negative": 0}
                    for detection in validated_detections:
                        accuracy = detection["validation"].get("accuracy", "unknown")
                        if accuracy in accuracy_stats:
                            accuracy_stats[accuracy] += 1
                    
                    # å‡†ç¡®ç‡è®¡ç®—
                    total_verified = sum(accuracy_stats.values())
                    if total_verified > 0:
                        accuracy_rate = ((accuracy_stats["true_positive"] + accuracy_stats["true_negative"]) / total_verified) * 100
                        accuracy_info = f"{accuracy_rate:.1f}%"
                    else:
                        accuracy_info = "å¾…éªŒè¯"
                    
                    # ç»„ä»¶çŠ¶æ€å’Œå‹æµ‹ç»Ÿè®¡
                    components_status = []
                    
                    if load_test_process and load_test_process.poll() is None:
                        # å‹æµ‹è¿è¡Œä¸­ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        total_req = self.load_test_stats["total_requests"]
                        success_req = self.load_test_stats["successful_requests"]
                        if total_req > 0:
                            success_rate = (success_req / total_req) * 100
                            components_status.append(f"å‹æµ‹:è¿è¡Œ({success_req}/{total_req}, {success_rate:.1f}%)")
                        else:
                            components_status.append("å‹æµ‹:è¿è¡Œ(ç»Ÿè®¡ä¸­...)")
                    else:
                        components_status.append("å‹æµ‹:å¤±è´¥")
                        
                    if collection_process and collection_process.poll() is None:
                        components_status.append("é‡‡é›†:è¿è¡Œ")
                    else:
                        components_status.append("é‡‡é›†:å¤±è´¥")
                
                    if metrics_process and metrics_process.poll() is None:
                        components_status.append("æŒ‡æ ‡:è¿è¡Œ")
                    else:
                        components_status.append("æŒ‡æ ‡:å¤±è´¥")
                    
                    # æ„å»ºç›‘æ§çŠ¶æ€ä¿¡æ¯
                    status_info = [
                        f"è¿è¡Œæ—¶é—´: {elapsed_hours:.1f}h",
                        f"æ£€æµ‹æ¬¡æ•°: {detection_count}",
                        f"å¼‚å¸¸æ¬¡æ•°: {anomaly_count}",
                        f"å‡†ç¡®ç‡: {accuracy_info}",
                        f"éªŒè¯: {len(validated_detections)}/{detection_count}",
                        " | ".join(components_status)
                    ]
                    
                    self.logger.info(f"ç›‘æ§çŠ¶æ€: {' | '.join(status_info)}")
                    last_status_display = current_time
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥ï¼ˆçŸ­é—´éš”ï¼‰
                time.sleep(check_interval)
        
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç›‘æ§")
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¼‚å¸¸: {e}")
        finally:
            self.monitoring_active = False
            
            # åœæ­¢æ‰€æœ‰åå°è¿›ç¨‹
            self._cleanup_processes(load_test_process, collection_process, metrics_process)
            
            self.results["end_time"] = datetime.now().isoformat()
            
            self.show_final_summary()

    def show_final_summary(self):
        """æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦"""
        detection_count = len(self.results["real_time_detections"])
        anomaly_count = len([d for d in self.results["real_time_detections"] if d["result"].get("anomaly_detected", False)])
        
        # è®¡ç®—éªŒè¯ç»Ÿè®¡
        validated_detections = [d for d in self.results["real_time_detections"] 
                              if d["validation"].get("validation") == "completed"]
        
        accuracy_stats = {"true_positive": 0, "true_negative": 0, "false_positive": 0, "false_negative": 0}
        for detection in validated_detections:
            accuracy = detection["validation"].get("accuracy", "unknown")
            if accuracy in accuracy_stats:
                accuracy_stats[accuracy] += 1
        
        total_validated = sum(accuracy_stats.values())
        accuracy_rate = ((accuracy_stats["true_positive"] + accuracy_stats["true_negative"]) / max(total_validated, 1)) * 100
        
        self.logger.info("=" * 50)
        self.logger.info("ç›‘æ§å®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        self.logger.info("=" * 50)
        self.logger.info(f"æ€»æ£€æµ‹æ¬¡æ•°: {detection_count}")
        self.logger.info(f"æ£€æµ‹åˆ°å¼‚å¸¸: {anomaly_count}")
        if total_validated > 0:
            self.logger.info(f"æ£€æµ‹å‡†ç¡®ç‡: {accuracy_rate:.1f}%")
            self.logger.info(f"  çœŸé˜³æ€§ (æ­£ç¡®æ£€æµ‹å¼‚å¸¸): {accuracy_stats['true_positive']}")
            self.logger.info(f"  çœŸé˜´æ€§ (æ­£ç¡®è¯†åˆ«æ­£å¸¸): {accuracy_stats['true_negative']}")
            self.logger.info(f"  å‡é˜³æ€§ (è¯¯æŠ¥): {accuracy_stats['false_positive']}")
            self.logger.info(f"  å‡é˜´æ€§ (æ¼æŠ¥): {accuracy_stats['false_negative']}")
        
        # æ˜¾ç¤ºå‹æµ‹æœ€ç»ˆç»Ÿè®¡
        total_req = self.load_test_stats["total_requests"]
        success_req = self.load_test_stats["successful_requests"]
        if total_req > 0:
            success_rate = (success_req / total_req) * 100
            self.logger.info(f"å‹æµ‹æœ€ç»ˆç»Ÿè®¡: {success_req}/{total_req} æˆåŠŸç‡: {success_rate:.1f}%")
        
        self.logger.info("=" * 50)

    def _cleanup_processes(self, *processes):
        """æ¸…ç†åå°è¿›ç¨‹"""
        process_names = ["å‹æµ‹", "æ•°æ®é‡‡é›†", "æŒ‡æ ‡é‡‡é›†"]
        
        for i, process in enumerate(processes):
            if process:
                name = process_names[i] if i < len(process_names) else f"è¿›ç¨‹{i}"
                self.logger.info(f"åœæ­¢{name}...")
                try:
                    process.terminate()
                    process.wait(timeout=30)
                    self.logger.info(f"{name}å·²åœæ­¢")
                except subprocess.TimeoutExpired:
                    self.logger.warning(f"{name}æœªå“åº”ï¼Œå¼ºåˆ¶ç»“æŸ")
                    try:
                        process.kill()
                        self.logger.info(f"{name}å·²å¼ºåˆ¶åœæ­¢")
                    except:
                        self.logger.error(f"æ— æ³•åœæ­¢{name}")
                except Exception as e:
                    self.logger.error(f"åœæ­¢{name}æ—¶å‡ºé”™: {e}")

    def process_new_files_real_time(self, csv_files: List[str]) -> bool:
        """å®æ—¶å¤„ç†æ–°çš„CSVæ–‡ä»¶"""
        if not csv_files:
            return False
        
        self.logger.info(f"å¤„ç†æ–°æ•°æ®æ–‡ä»¶: {len(csv_files)} ä¸ª")
        
        # å¯¹æ¯ä¸ªæ–°æ–‡ä»¶è¿›è¡Œæ ‡ç­¾ç”Ÿæˆå’Œå¼‚å¸¸æ£€æµ‹
        for csv_file in csv_files:
            try:
                # å…ˆè¿›è¡Œæ ‡ç­¾ç”Ÿæˆå¤„ç†
                self.logger.debug(f"æ ‡ç­¾ç”Ÿæˆ: {os.path.basename(csv_file)}")
                success = self.process_collected_data(csv_file_path=csv_file)
                
                if not success:
                    self.logger.warning(f"æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {os.path.basename(csv_file)}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å¤„ç†æ ‡å¿—
                flag_file = csv_file.replace('.csv', '.label_processed')
                if not os.path.exists(flag_file):
                    self.logger.warning(f"æ ‡ç­¾å¤„ç†æ ‡å¿—æ–‡ä»¶ç¼ºå¤±: {os.path.basename(flag_file)}")
                    continue
                
                # è°ƒç”¨å¼‚å¸¸æ£€æµ‹
                detection_result = self.run_anomaly_detection(csv_file)
                
                # éªŒè¯ç»“æœ
                validation = self.validate_detection_result(detection_result)
                
                # æŠ¥å‘Šæ£€æµ‹ç»“æœ
                self.report_anomaly_detection(csv_file, detection_result, validation)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {csv_file}: {e}")
        
        return True

    def get_expected_anomaly_from_csv(self, csv_file: str) -> Dict:
        """ä»CSVæ–‡ä»¶çš„æ ‡ç­¾åˆ—è¯»å–æœŸæœ›çš„å¼‚å¸¸ç»“æœ"""
        try:
            import pandas as pd
            df = pd.read_csv(csv_file)
            
            # æ£€æŸ¥æ˜¯å¦ä¸º14åˆ—æ ¼å¼ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰
            if len(df.columns) != 14:
                return {
                    "has_labels": False,
                    "reason": f"CSVæ–‡ä»¶åˆ—æ•°ä¸åŒ¹é…ï¼ŒæœŸæœ›14åˆ—ï¼Œå®é™…{len(df.columns)}åˆ—"
                }
            
            # æ£€æŸ¥æ ‡ç­¾åˆ—æ˜¯å¦å­˜åœ¨
            label_columns = ['nodeLatencyLabel', 'graphLatencyLabel', 'graphStructureLabel']
            missing_columns = [col for col in label_columns if col not in df.columns]
            
            if missing_columns:
                return {
                    "has_labels": False,
                    "reason": f"ç¼ºå°‘æ ‡ç­¾åˆ—: {missing_columns}"
                }
            
            # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
            node_labels = df['nodeLatencyLabel'].value_counts().to_dict()
            graph_labels = df['graphLatencyLabel'].value_counts().to_dict()
            
            # åˆ¤æ–­æ˜¯å¦æœŸæœ›å¼‚å¸¸ï¼šä»»ä¸€æ ‡ç­¾æœ‰å¼‚å¸¸å€¼(1)åˆ™æœŸæœ›å¼‚å¸¸
            has_node_anomaly = node_labels.get(1, 0) > 0
            has_graph_anomaly = graph_labels.get(1, 0) > 0
            expected_anomaly = has_node_anomaly or has_graph_anomaly
            
            return {
                "has_labels": True,
                "expected_anomaly": expected_anomaly,
                "total_spans": len(df),
                "unique_traces": len(df['traceIdLow'].unique()) if 'traceIdLow' in df.columns else 0,
                "label_distribution": {
                    "nodeLatencyLabel": node_labels,
                    "graphLatencyLabel": graph_labels,
                    "anomaly_spans": node_labels.get(1, 0) + graph_labels.get(1, 0)
                },
                "anomaly_reason": "æ ‡ç­¾æ˜¾ç¤ºå¼‚å¸¸" if expected_anomaly else "æ ‡ç­¾æ˜¾ç¤ºæ­£å¸¸"
            }
            
        except Exception as e:
            return {
                "has_labels": False,
                "reason": f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}"
            }

    # def call_anomaly_detection_model(self, csv_file: str) -> Dict:
    #     """è°ƒç”¨å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆå½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°ï¼‰"""
    #     self.logger.debug(f"å¼‚å¸¸æ£€æµ‹: {os.path.basename(csv_file)}")
        
    #     # TODO: è¿™é‡Œå¯ä»¥æ›¿æ¢ä¸ºçœŸå®çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹è°ƒç”¨
    #     # ä¾‹å¦‚ï¼šresult = subprocess.run([python, model_script, csv_file])
        
    #     script_path = self.project_root / self.config["scripts"]["anomaly_detection"]
        
    #     if not os.path.exists(script_path):
    #         self.logger.debug(f"ä½¿ç”¨æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹ (çœŸå®æ¨¡å‹è·¯å¾„: {script_path})")
        
    #     # è¯»å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ç”¨äºæ¨¡æ‹Ÿ
    #     try:
    #         import pandas as pd
    #         df = pd.read_csv(csv_file)
    #         trace_count = len(df['traceIdLow'].unique()) if 'traceIdLow' in df.columns else 0
    #         span_count = len(df)
    #     except Exception as e:
    #         self.logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
    #         trace_count = 0
    #         span_count = 0
        
    #     # æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹ç»“æœ
    #     import random
    #     anomaly_score = random.uniform(0.0, 1.0)
    #     threshold = self.config["real_time_mode"]["detection_threshold"]
    #     is_anomaly = anomaly_score > threshold
        
    #     result = {
    #         "file_name": os.path.basename(csv_file),
    #         "analysis_time": datetime.now().isoformat(),
    #         "trace_count": trace_count, 
    #         "span_count": span_count,
    #         "anomaly_score": round(anomaly_score, 4),
    #         "threshold": threshold,
    #         "anomaly_detected": is_anomaly,
    #         "model_confidence": round(abs(anomaly_score - threshold), 4),
    #         "anomaly_types": []
    #     }
        
    #     if is_anomaly:
    #         possible_anomalies = ["high_latency", "error_spike", "unusual_pattern", "service_degradation"]
    #         result["anomaly_types"] = random.sample(possible_anomalies, random.randint(1, 2))
            
    #     return result

    def call_anomaly_detection_model(self, csv_file: str) -> Dict:
        """è°ƒç”¨TraceVAEå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        self.logger.debug(f"å¼‚å¸¸æ£€æµ‹: {os.path.basename(csv_file)}")
        
        try:
            # ä½¿ç”¨TraceVAEåœ¨çº¿æ£€æµ‹
            result = self._call_tracevae_online_detection(csv_file)
            if result:
                return result
        except Exception as e:
            self.logger.warning(f"TraceVAEåœ¨çº¿æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ¨¡å¼: {e}")
        
        # é™çº§åˆ°åŸæœ‰çš„æ¨¡æ‹Ÿæ£€æµ‹
        return self._fallback_simulation_detection(csv_file)

    def _call_tracevae_online_detection(self, csv_file: str) -> Dict:
        """è°ƒç”¨TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡"""
        import asyncio
        import sys
        
        # æ·»åŠ csv_file_monitor.pyæ‰€åœ¨è·¯å¾„ï¼ˆå‡è®¾åœ¨åŒä¸€ç›®å½•æˆ–å­ç›®å½•ï¼‰
        monitor_path = str(self.project_root)  # æˆ–è€…å…·ä½“çš„è·¯å¾„
        if monitor_path not in sys.path:
            sys.path.append(monitor_path)
        
        # å¯¼å…¥CSVç›‘æ§å™¨
        from csv_file_monitor import CSVFileMonitor
        
        # TraceVAEæœåŠ¡URLï¼ˆå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼‰
        tracevae_url = os.getenv("TRACEVAE_SERVICE_URL", "http://localhost:8000")
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = CSVFileMonitor(tracevae_url)
        
        # å¼‚æ­¥è°ƒç”¨æ£€æµ‹
        async def detect():
            return await monitor.process_entire_csv_file(csv_file, batch_size=20)
        
        # è¿è¡Œæ£€æµ‹
        tracevae_result = asyncio.run(detect())
        
        # è½¬æ¢ä¸ºTrain TicketæœŸæœ›çš„æ ¼å¼
        return self._convert_tracevae_to_train_ticket_format(tracevae_result, csv_file)

    def _convert_tracevae_to_train_ticket_format(self, tracevae_result: Dict, csv_file: str) -> Dict:
        """å°†TraceVAEç»“æœè½¬æ¢ä¸ºTrain TicketæœŸæœ›çš„æ ¼å¼"""
        try:
            file_name = os.path.basename(csv_file)
            
            # æå–TraceVAEç»“æœ
            status = tracevae_result.get('status', 'NORMAL')
            total_traces = tracevae_result.get('total_traces', 0)
            anomaly_traces = tracevae_result.get('anomaly_traces', 0)
            anomaly_percentage = tracevae_result.get('anomaly_percentage', 0.0)
            
            # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
            is_anomaly = status == 'ANOMALY'
            
            # è®¡ç®—å¼‚å¸¸åˆ†æ•° (0.0-1.0)
            anomaly_score = anomaly_percentage / 100.0 if total_traces > 0 else 0.0
            
            # è·å–é˜ˆå€¼
            threshold = self.config["real_time_mode"]["detection_threshold"]
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = abs(anomaly_score - threshold)
            
            # æ„å»ºå¼‚å¸¸ç±»å‹
            anomaly_types = []
            if is_anomaly:
                if anomaly_percentage > 80:
                    anomaly_types.append("high_anomaly_rate")
                elif anomaly_percentage > 50:
                    anomaly_types.append("moderate_anomaly_rate")
                else:
                    anomaly_types.append("low_anomaly_rate")
                
                # æ·»åŠ TraceVAEç‰¹å®šçš„å¼‚å¸¸ç±»å‹
                anomaly_details = tracevae_result.get('anomaly_types', {})
                for anomaly_type in anomaly_details.keys():
                    anomaly_types.append(f"tracevae_{anomaly_type}")
            
            return {
                "file_name": file_name,
                "analysis_time": datetime.now().isoformat(),
                "trace_count": total_traces,
                "span_count": total_traces,  # ç®€åŒ–å¤„ç†
                "anomaly_score": round(anomaly_score, 4),
                "threshold": threshold,
                "anomaly_detected": is_anomaly,
                "model_confidence": round(confidence, 4),
                "anomaly_types": anomaly_types,
                "detection_method": "tracevae_online",
                "tracevae_details": {
                    "status": status,
                    "total_traces": total_traces,
                    "anomaly_traces": anomaly_traces,
                    "normal_traces": tracevae_result.get('normal_traces', 0),
                    "anomaly_percentage": anomaly_percentage,
                    "anomaly_types": tracevae_result.get('anomaly_types', {}),
                    "detection_timestamp": tracevae_result.get('detection_timestamp')
                }
            }
            
        except Exception as e:
            self.logger.error(f"TraceVAEç»“æœè½¬æ¢å¤±è´¥: {e}")
            raise Exception(f"ç»“æœè½¬æ¢å¤±è´¥: {e}")

    def _fallback_simulation_detection(self, csv_file: str) -> Dict:
        """é™çº§åˆ°åŸæœ‰çš„æ¨¡æ‹Ÿæ£€æµ‹"""
        self.logger.info(f"ä½¿ç”¨æ¨¡æ‹Ÿæ£€æµ‹: {os.path.basename(csv_file)}")
        
        # è¿™é‡Œæ˜¯åŸæ¥çš„æ¨¡æ‹Ÿæ£€æµ‹é€»è¾‘
        script_path = self.project_root / self.config["scripts"]["anomaly_detection"]
        
        if not os.path.exists(script_path):
            self.logger.debug(f"ä½¿ç”¨æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹ (çœŸå®æ¨¡å‹è·¯å¾„: {script_path})")
        
        # è¯»å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯ç”¨äºæ¨¡æ‹Ÿ
        try:
            import pandas as pd
            df = pd.read_csv(csv_file)
            trace_count = len(df['traceIdLow'].unique()) if 'traceIdLow' in df.columns else 0
            span_count = len(df)
        except Exception as e:
            self.logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            trace_count = 0
            span_count = 0
        
        # æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹ç»“æœ
        import random
        anomaly_score = random.uniform(0.0, 1.0)
        threshold = self.config["real_time_mode"]["detection_threshold"]
        is_anomaly = anomaly_score > threshold
        
        result = {
            "file_name": os.path.basename(csv_file),
            "analysis_time": datetime.now().isoformat(),
            "trace_count": trace_count, 
            "span_count": span_count,
            "anomaly_score": round(anomaly_score, 4),
            "threshold": threshold,
            "anomaly_detected": is_anomaly,
            "model_confidence": round(abs(anomaly_score - threshold), 4),
            "anomaly_types": [],
            "detection_method": "simulation"
        }
        
        if is_anomaly:
            possible_anomalies = ["high_latency", "error_spike", "unusual_pattern", "service_degradation"]
            result["anomaly_types"] = random.sample(possible_anomalies, random.randint(1, 2))
            
        return result



    def validate_detection_accuracy(self, expected_anomaly: bool, detected_anomaly: bool, 
                                   label_info: Dict, detection_result: Dict) -> Dict:
        """éªŒè¯æ£€æµ‹å‡†ç¡®æ€§"""
        # è®¡ç®—å‡†ç¡®æ€§ç±»åˆ«
        if expected_anomaly and detected_anomaly:
            accuracy = "true_positive"
            result_desc = "æ­£ç¡®æ£€æµ‹åˆ°å¼‚å¸¸"
        elif not expected_anomaly and not detected_anomaly:
            accuracy = "true_negative"
            result_desc = "æ­£ç¡®è¯†åˆ«æ­£å¸¸"
        elif not expected_anomaly and detected_anomaly:
            accuracy = "false_positive"
            result_desc = "è¯¯æŠ¥å¼‚å¸¸"
        else:  # expected_anomaly and not detected_anomaly
            accuracy = "false_negative"
            result_desc = "æ¼æŠ¥å¼‚å¸¸"
        
        validation_result = {
            "validation": "completed",
            "expected_anomaly": expected_anomaly,
            "detected_anomaly": detected_anomaly,
            "accuracy": accuracy,
            "result": f"{accuracy}: {result_desc}",
            "label_info": {
                "total_spans": label_info.get("total_spans", 0),
                "unique_traces": label_info.get("unique_traces", 0),
                "anomaly_spans": label_info.get("label_distribution", {}).get("anomaly_spans", 0),
                "anomaly_reason": label_info.get("anomaly_reason", "æœªçŸ¥")
            },
            "model_info": {
                "anomaly_score": detection_result.get("anomaly_score", 0),
                "threshold": detection_result.get("threshold", 0),
                "confidence": detection_result.get("model_confidence", 0)
            }
        }
        
        return validation_result

    def run_anomaly_detection(self, csv_file: str) -> Dict:
        """è¿è¡Œå¼‚å¸¸æ£€æµ‹æµç¨‹"""
        # 1. è¯»å–CSVæ ‡ç­¾è·å–æœŸæœ›ç»“æœ
        label_info = self.get_expected_anomaly_from_csv(csv_file)
        
        if not label_info.get("has_labels", False):
            return {
                "file_name": os.path.basename(csv_file),
                "error": "æ— æ³•è¯»å–æ ‡ç­¾",
                "reason": label_info.get("reason", "æœªçŸ¥é”™è¯¯"),
                "anomaly_detected": False
            }
        
        # 2. è°ƒç”¨å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        detection_result = self.call_anomaly_detection_model(csv_file)
        
        # 3. éªŒè¯æ£€æµ‹å‡†ç¡®æ€§
        expected_anomaly = label_info.get("expected_anomaly", False)
        detected_anomaly = detection_result.get("anomaly_detected", False)
        
        validation = self.validate_detection_accuracy(
            expected_anomaly, detected_anomaly, label_info, detection_result
        )
        
        # 4. åˆå¹¶ç»“æœ
        detection_result["validation"] = validation
        
        return detection_result

    def validate_detection_result(self, detection_result: Dict) -> Dict:
        """æå–éªŒè¯ç»“æœï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        return detection_result.get("validation", {
            "validation": "failed",
            "reason": "æ— éªŒè¯ä¿¡æ¯"
        })

    def report_anomaly_detection(self, csv_file: str, detection_result: Dict, validation: Dict):
        """æŠ¥å‘Šå¼‚å¸¸æ£€æµ‹ç»“æœ"""
        file_name = os.path.basename(csv_file)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in detection_result:
            self.logger.error(f"æ£€æµ‹å¤±è´¥ {file_name}: {detection_result.get('reason', 'æœªçŸ¥é”™è¯¯')}")
            return
        
        detected_anomaly = detection_result.get("anomaly_detected", False)
        expected_anomaly = validation.get("expected_anomaly", False)
        accuracy = validation.get("accuracy", "unknown")
        
        # æ„å»ºç»“æœä¿¡æ¯
        model_info = validation.get("model_info", {})
        label_info = validation.get("label_info", {})
        
        anomaly_score = model_info.get("anomaly_score", 0)
        threshold = model_info.get("threshold", 0)
        confidence = model_info.get("confidence", 0)
        
        total_spans = label_info.get("total_spans", 0)
        unique_traces = label_info.get("unique_traces", 0)
        anomaly_spans = label_info.get("anomaly_spans", 0)
        
        if detected_anomaly:
            # æ£€æµ‹åˆ°å¼‚å¸¸
            status_icon = "âœ“" if accuracy in ["true_positive"] else "âœ—"
            self.logger.warning("=" * 60)
            self.logger.warning(f"å¼‚å¸¸æ£€æµ‹ {status_icon} {file_name}")
            self.logger.warning("=" * 60)
            self.logger.warning(f"æ¨¡å‹ç»“æœ: å¼‚å¸¸ (åˆ†æ•°: {anomaly_score}, é˜ˆå€¼: {threshold}, ç½®ä¿¡åº¦: {confidence})")
            self.logger.warning(f"æ ‡ç­¾æœŸæœ›: {'å¼‚å¸¸' if expected_anomaly else 'æ­£å¸¸'} (å¼‚å¸¸span: {anomaly_spans}/{total_spans})")
            self.logger.warning(f"å‡†ç¡®æ€§è¯„ä¼°: {validation.get('result', 'æœªçŸ¥')}")
            self.logger.warning(f"æ•°æ®ç»Ÿè®¡: {unique_traces} traces, {total_spans} spans")
            
            if detection_result.get("anomaly_types"):
                self.logger.warning(f"å¼‚å¸¸ç±»å‹: {', '.join(detection_result['anomaly_types'])}")
            
            self.logger.warning("=" * 60)
        else:
            # æ£€æµ‹ä¸ºæ­£å¸¸
            status_icon = "âœ“" if accuracy in ["true_negative"] else "âœ—"
            status = "CORRECT" if accuracy in ["true_negative", "true_positive"] else "INCORRECT"
            
            self.logger.info(f"æ­£å¸¸æ£€æµ‹ {status_icon} {file_name} - {status}")
            self.logger.info(f"  æ¨¡å‹: æ­£å¸¸ (åˆ†æ•°: {anomaly_score}) | æ ‡ç­¾: {'å¼‚å¸¸' if expected_anomaly else 'æ­£å¸¸'} | {validation.get('result', 'æœªçŸ¥')}")
        
        # ä¿å­˜åˆ°å†…å­˜ä¸­çš„ç»“æœåˆ—è¡¨
        self.results["real_time_detections"].append({
            "timestamp": datetime.now().isoformat(),
            "file": file_name,
            "result": detection_result,
            "validation": validation
        })

    def process_detection_json_results(self, target_date: str = None):
        """å¤„ç†æ£€æµ‹ç»“æœJSONæ–‡ä»¶å¹¶è¾“å‡ºåˆ°æ§åˆ¶å°"""
        target_date = target_date or datetime.now().strftime("%Y-%m-%d")
        
        # æŸ¥æ‰¾JSONç»“æœæ–‡ä»¶
        project_dir = self.project_root
        json_files = list(project_dir.glob("*.json"))
        
        # è¿‡æ»¤å‡ºæ£€æµ‹ç»“æœæ–‡ä»¶ï¼ˆæ ¹æ®æ–‡ä»¶åæ¨¡å¼ï¼‰
        detection_json_files = []
        for json_file in json_files:
            # å‡è®¾æ£€æµ‹ç»“æœæ–‡ä»¶åæ ¼å¼ä¸º æ—¶é—´.json (å¦‚ 19_09.json)
            if json_file.stem.count('_') == 1 and json_file.stem.replace('_', '').isdigit():
                detection_json_files.append(json_file)
        
        if not detection_json_files:
            self.logger.info("æœªæ‰¾åˆ°æ£€æµ‹ç»“æœJSONæ–‡ä»¶")
            return
        
        # æŒ‰æ–‡ä»¶åæ’åº
        detection_json_files.sort()
        
        self.logger.info(f"æ‰¾åˆ° {len(detection_json_files)} ä¸ªæ£€æµ‹ç»“æœæ–‡ä»¶")
        self.logger.info("=" * 80)
        
        total_files = 0
        anomaly_files = 0
        normal_files = 0
        
        for json_file in detection_json_files:
            try:
                result_data = self.read_detection_json(json_file)
                if result_data:
                    self.display_detection_result(json_file.name, result_data)
                    
                    total_files += 1
                    if result_data.get('status') == 'ANOMALY':
                        anomaly_files += 1
                    else:
                        normal_files += 1
                        
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
        
        # è¾“å‡ºæ±‡æ€»ç»Ÿè®¡
        self.display_summary_statistics(total_files, anomaly_files, normal_files)

    def read_detection_json(self, json_file_path: Path) -> Dict:
        """è¯»å–æ£€æµ‹ç»“æœJSONæ–‡ä»¶"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            self.logger.error(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥ {json_file_path}: {e}")
            return {}

    def display_detection_result(self, filename: str, result_data: Dict):
        """æ˜¾ç¤ºå•ä¸ªæ£€æµ‹ç»“æœ"""
        status = result_data.get('status', 'UNKNOWN')
        csv_file = result_data.get('csv_file', filename.replace('.json', '.csv'))
        total_traces = result_data.get('total_traces', 0)
        anomaly_traces = result_data.get('anomaly_traces', 0)
        normal_traces = result_data.get('normal_traces', 0)
        anomaly_percentage = result_data.get('anomaly_percentage', 0.0)
        anomaly_threshold = result_data.get('anomaly_threshold', 0.2)
        detection_timestamp = result_data.get('detection_timestamp', 'Unknown')
        
        # çŠ¶æ€å›¾æ ‡å’Œé¢œè‰²
        if status == 'ANOMALY':
            status_icon = "ğŸš¨"
            status_color = "å¼‚å¸¸"
        else:
            status_icon = "âœ…"
            status_color = "æ­£å¸¸"
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        print(f"\n{status_icon} æ–‡ä»¶: {csv_file}")
        print(f"ğŸ“‹ æ£€æµ‹çŠ¶æ€: {status_color}")
        print(f"ğŸ“Š Tracesç»Ÿè®¡: æ€»è®¡ {total_traces} | æ­£å¸¸ {normal_traces} | å¼‚å¸¸ {anomaly_traces}")
        print(f"ğŸ“ˆ å¼‚å¸¸æ¯”ä¾‹: {anomaly_percentage:.2f}% (é˜ˆå€¼: {anomaly_threshold}%)")
        print(f"ğŸ•’ æ£€æµ‹æ—¶é—´: {detection_timestamp}")
        
        # å¦‚æœæœ‰å¼‚å¸¸ï¼Œæ˜¾ç¤ºå¼‚å¸¸ç±»å‹
        if status == 'ANOMALY':
            anomaly_types = result_data.get('anomaly_types', {})
            if anomaly_types:
                print(f"ğŸ·ï¸  å¼‚å¸¸ç±»å‹:")
                for anomaly_type, count in anomaly_types.items():
                    print(f"   - {anomaly_type}: {count} æ¬¡")
            
            # æ˜¾ç¤ºå¼‚å¸¸è¯¦æƒ…ç¤ºä¾‹
            anomaly_details = result_data.get('anomaly_details', [])
            if anomaly_details:
                print(f"ğŸ” å¼‚å¸¸ç¤ºä¾‹:")
                for i, detail in enumerate(anomaly_details[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    trace_id = detail.get('traceID', 'Unknown')
                    anomaly_type = detail.get('anomaly_type', 'Unknown')
                    confidence = detail.get('confidence', 0)
                    print(f"   {i+1}. {trace_id}: {anomaly_type} (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        print("-" * 60)

    def display_summary_statistics(self, total_files: int, anomaly_files: int, normal_files: int):
        """æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æ£€æµ‹ç»“æœæ±‡æ€»ç»Ÿè®¡")
        print("=" * 80)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"ğŸš¨ å¼‚å¸¸æ–‡ä»¶: {anomaly_files}")
        print(f"âœ… æ­£å¸¸æ–‡ä»¶: {normal_files}")
        
        if total_files > 0:
            anomaly_rate = (anomaly_files / total_files) * 100
            print(f"ğŸ“ˆ å¼‚å¸¸æ–‡ä»¶æ¯”ä¾‹: {anomaly_rate:.1f}%")
        
        print("=" * 80)

    def run_json_analysis_mode(self):
        """è¿è¡ŒJSONåˆ†ææ¨¡å¼"""
        self.logger.info("å¯åŠ¨JSONæ£€æµ‹ç»“æœåˆ†ææ¨¡å¼")
        
        try:
            self.process_detection_json_results()
        except Exception as e:
            self.logger.error(f"JSONåˆ†æå¤±è´¥: {e}")

    def check_tracevae_service(self) -> bool:
        """æ£€æŸ¥TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = requests.get(f"{self.tracevae_service_url}/health", timeout=5)
            if response.status_code == 200:
                self.logger.info("TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡è¿è¡Œæ­£å¸¸")
                return True
            else:
                self.logger.warning(f"TraceVAEæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"æ— æ³•è¿æ¥åˆ°TraceVAEæœåŠ¡: {e}")
            return False

    def start_tracevae_service(self) -> bool:
        """å¯åŠ¨TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡"""
        tracevae_dir = self.project_root / "tracevae" / "online_detector"
        run_script = tracevae_dir / "run.py"
        
        if not tracevae_dir.exists():
            self.logger.error(f"TraceVAEç›®å½•ä¸å­˜åœ¨: {tracevae_dir}")
            return False
        
        if not run_script.exists():
            self.logger.error(f"TraceVAEå¯åŠ¨è„šæœ¬ä¸å­˜åœ¨: {run_script}")
            return False
        
        try:
            self.logger.info("å¯åŠ¨TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡...")
            
            # å¯åŠ¨æœåŠ¡
            self.tracevae_service_process = subprocess.Popen(
                [sys.executable, "run.py"],
                cwd=str(tracevae_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True
            )
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            max_wait_time = 30  # æœ€å¤šç­‰å¾…30ç§’
            wait_interval = 2   # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
            
            for i in range(max_wait_time // wait_interval):
                time.sleep(wait_interval)
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                if self.tracevae_service_process.poll() is not None:
                    # è¿›ç¨‹å·²é€€å‡ºï¼Œè¯»å–é”™è¯¯ä¿¡æ¯
                    _, stderr = self.tracevae_service_process.communicate()
                    self.logger.error(f"TraceVAEæœåŠ¡å¯åŠ¨å¤±è´¥: {stderr}")
                    return False
                
                # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
                if self.check_tracevae_service():
                    self.logger.info("TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡å¯åŠ¨æˆåŠŸ")
                    self.component_status["tracevae_service"] = "è¿è¡Œä¸­"
                    return True
                
                self.logger.info(f"ç­‰å¾…TraceVAEæœåŠ¡å¯åŠ¨... ({i+1}/{max_wait_time//wait_interval})")
            
            # è¶…æ—¶æœªå¯åŠ¨æˆåŠŸ
            self.logger.error("TraceVAEæœåŠ¡å¯åŠ¨è¶…æ—¶")
            self.stop_tracevae_service()
            return False
            
        except Exception as e:
            self.logger.error(f"å¯åŠ¨TraceVAEæœåŠ¡å¼‚å¸¸: {e}")
            return False

    def stop_tracevae_service(self):
        """åœæ­¢TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡"""
        if self.tracevae_service_process:
            try:
                self.logger.info("åœæ­¢TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡...")
                self.tracevae_service_process.terminate()
                self.tracevae_service_process.wait(timeout=10)
                self.logger.info("TraceVAEæœåŠ¡å·²åœæ­¢")
            except subprocess.TimeoutExpired:
                self.logger.warning("TraceVAEæœåŠ¡æœªå“åº”ï¼Œå¼ºåˆ¶ç»“æŸ")
                self.tracevae_service_process.kill()
            except Exception as e:
                self.logger.error(f"åœæ­¢TraceVAEæœåŠ¡æ—¶å‡ºé”™: {e}")
            finally:
                self.tracevae_service_process = None
                self.component_status["tracevae_service"] = "å·²åœæ­¢"

    def ensure_tracevae_service_running(self) -> bool:
        """ç¡®ä¿TraceVAEæœåŠ¡æ­£åœ¨è¿è¡Œ"""
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
        if self.check_tracevae_service():
            return True
        
        # æœåŠ¡æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨
        self.logger.info("TraceVAEæœåŠ¡æœªè¿è¡Œï¼Œæ­£åœ¨å¯åŠ¨...")
        return self.start_tracevae_service()

    def _call_tracevae_online_detection(self, csv_file: str) -> Dict:
        """è°ƒç”¨TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡"""
        # ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ
        if not self.ensure_tracevae_service_running():
            raise Exception("TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡ä¸å¯ç”¨")
        
        import asyncio
        import sys
        
        # æ·»åŠ csv_file_monitor.pyæ‰€åœ¨è·¯å¾„
        monitor_path = str(self.project_root)
        if monitor_path not in sys.path:
            sys.path.append(monitor_path)
        
        # å¯¼å…¥CSVç›‘æ§å™¨
        from csv_file_monitor import CSVFileMonitor
        
        # åˆ›å»ºç›‘æ§å™¨
        monitor = CSVFileMonitor(self.tracevae_service_url)
        
        # å¼‚æ­¥è°ƒç”¨æ£€æµ‹
        async def detect():
            return await monitor.process_entire_csv_file(csv_file, batch_size=20)
        
        # è¿è¡Œæ£€æµ‹
        tracevae_result = asyncio.run(detect())
        
        # è½¬æ¢ä¸ºTrain TicketæœŸæœ›çš„æ ¼å¼
        return self._convert_tracevae_to_train_ticket_format(tracevae_result, csv_file)

    def run_real_time_monitoring(self):
        """è¿è¡Œå®æ—¶ç›‘æ§æ¨¡å¼"""
        self.logger.info("å¯åŠ¨å®æ—¶å¼‚å¸¸æ£€æµ‹ç›‘æ§ç³»ç»Ÿ")
        self.logger.info("ç³»ç»Ÿé…ç½®:")
        self.logger.info(f"  - æ£€æµ‹é˜ˆå€¼: {self.config['real_time_mode']['detection_threshold']}")
        self.logger.info(f"  - æ£€æŸ¥é—´éš”: {self.config['real_time_mode']['check_interval_seconds']} ç§’")
        self.logger.info(f"  - çŠ¶æ€æ˜¾ç¤ºé—´éš”: {self.config['real_time_mode']['status_display_interval_seconds']} ç§’")
        self.logger.info(f"è„šæœ¬å¯åŠ¨æ—¶é—´: {self.script_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.monitoring_active = True
        self.results["start_time"] = datetime.now().isoformat()
        
        # é¦–å…ˆç¡®ä¿TraceVAEæœåŠ¡è¿è¡Œ
        self.logger.info("æ£€æŸ¥TraceVAEåœ¨çº¿æ£€æµ‹æœåŠ¡...")
        if not self.ensure_tracevae_service_running():
            self.logger.error("TraceVAEæœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹")
            self.monitoring_active = False
            return
        
        # å¯åŠ¨å‹æµ‹
        load_test_process = self.start_load_test()
        if not load_test_process:
            self.logger.warning("å‹æµ‹å¯åŠ¨å¤±è´¥ï¼Œç»§ç»­ç›‘æ§")
        
        # å¯åŠ¨æ•°æ®é‡‡é›†
        collection_process = self.start_data_collection(duration_minutes=0)
        
        # å¯åŠ¨æŒ‡æ ‡é‡‡é›†
        metrics_process = self.start_metrics_collection(duration_minutes=0)
        if not metrics_process:
            self.logger.warning("æŒ‡æ ‡é‡‡é›†å¯åŠ¨å¤±è´¥ï¼Œä½†å°†ç»§ç»­ç›‘æ§")
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶æ˜¯å¦æˆåŠŸå¯åŠ¨
        if not collection_process:
            self.logger.error("æ•°æ®é‡‡é›†å¯åŠ¨å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ç›‘æ§")
            self.monitoring_active = False
            return
        
        # å¯åŠ¨ç›‘æ§å¾ªç¯
        check_interval = self.config["real_time_mode"]["check_interval_seconds"]
        status_display_interval = self.config["real_time_mode"]["status_display_interval_seconds"]
        self.logger.info(f"å¼€å§‹å®æ—¶ç›‘æ§ (æ£€æŸ¥é—´éš”: {check_interval}s, çŠ¶æ€æ˜¾ç¤ºé—´éš”: {status_display_interval}s)")
        
        start_time = time.time()
        last_status_display = 0
        
        try:
            while self.monitoring_active:
                current_time = time.time()
                elapsed_minutes = (current_time - start_time) / 60
                
                # æ£€æŸ¥æ–°æ•°æ®
                new_files = self.check_for_new_data()
                
                if new_files:
                    self.process_new_files_real_time(new_files)
                
                # åªåœ¨è¾¾åˆ°çŠ¶æ€æ˜¾ç¤ºé—´éš”æ—¶æ‰æ˜¾ç¤ºçŠ¶æ€
                if current_time - last_status_display >= status_display_interval:
                    elapsed_hours = elapsed_minutes / 60
                    detection_count = len(self.results["real_time_detections"])
                    anomaly_count = len([d for d in self.results["real_time_detections"] if d["result"].get("anomaly_detected", False)])
                    
                    # è®¡ç®—éªŒè¯ç»Ÿè®¡
                    all_detections = self.results["real_time_detections"]
                    validated_detections = [d for d in all_detections if d["validation"].get("validation") == "completed"]
                    
                    accuracy_stats = {"true_positive": 0, "true_negative": 0, "false_positive": 0, "false_negative": 0}
                    for detection in validated_detections:
                        accuracy = detection["validation"].get("accuracy", "unknown")
                        if accuracy in accuracy_stats:
                            accuracy_stats[accuracy] += 1
                    
                    # å‡†ç¡®ç‡è®¡ç®—
                    total_verified = sum(accuracy_stats.values())
                    if total_verified > 0:
                        accuracy_rate = ((accuracy_stats["true_positive"] + accuracy_stats["true_negative"]) / total_verified) * 100
                        accuracy_info = f"{accuracy_rate:.1f}%"
                    else:
                        accuracy_info = "å¾…éªŒè¯"
                    
                    # ç»„ä»¶çŠ¶æ€å’Œå‹æµ‹ç»Ÿè®¡
                    components_status = []
                    
                    # TraceVAEæœåŠ¡çŠ¶æ€
                    if self.check_tracevae_service():
                        components_status.append("æ£€æµ‹æœåŠ¡:è¿è¡Œ")
                    else:
                        components_status.append("æ£€æµ‹æœåŠ¡:å¼‚å¸¸")
                    
                    if load_test_process and load_test_process.poll() is None:
                        # å‹æµ‹è¿è¡Œä¸­ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        total_req = self.load_test_stats["total_requests"]
                        success_req = self.load_test_stats["successful_requests"]
                        if total_req > 0:
                            success_rate = (success_req / total_req) * 100
                            components_status.append(f"å‹æµ‹:è¿è¡Œ({success_req}/{total_req}, {success_rate:.1f}%)")
                        else:
                            components_status.append("å‹æµ‹:è¿è¡Œ(ç»Ÿè®¡ä¸­...)")
                    else:
                        components_status.append("å‹æµ‹:å¤±è´¥")
                        
                    if collection_process and collection_process.poll() is None:
                        components_status.append("é‡‡é›†:è¿è¡Œ")
                    else:
                        components_status.append("é‡‡é›†:å¤±è´¥")
                
                    if metrics_process and metrics_process.poll() is None:
                        components_status.append("æŒ‡æ ‡:è¿è¡Œ")
                    else:
                        components_status.append("æŒ‡æ ‡:å¤±è´¥")
                    
                    # æ„å»ºç›‘æ§çŠ¶æ€ä¿¡æ¯
                    status_info = [
                        f"è¿è¡Œæ—¶é—´: {elapsed_hours:.1f}h",
                        f"æ£€æµ‹æ¬¡æ•°: {detection_count}",
                        f"å¼‚å¸¸æ¬¡æ•°: {anomaly_count}",
                        f"å‡†ç¡®ç‡: {accuracy_info}",
                        f"éªŒè¯: {len(validated_detections)}/{detection_count}",
                        " | ".join(components_status)
                    ]
                    
                    self.logger.info(f"ç›‘æ§çŠ¶æ€: {' | '.join(status_info)}")
                    last_status_display = current_time
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥ï¼ˆçŸ­é—´éš”ï¼‰
                time.sleep(check_interval)
        
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­ç›‘æ§")
        except Exception as e:
            self.logger.error(f"ç›‘æ§å¼‚å¸¸: {e}")
        finally:
            self.monitoring_active = False
            
            # åœæ­¢æ‰€æœ‰åå°è¿›ç¨‹
            self._cleanup_processes(load_test_process, collection_process, metrics_process)
            
            # åœæ­¢TraceVAEæœåŠ¡
            self.stop_tracevae_service()
            
            self.results["end_time"] = datetime.now().isoformat()
            
            self.show_final_summary()

    def _cleanup_processes(self, *processes):
        """æ¸…ç†åå°è¿›ç¨‹"""
        process_names = ["å‹æµ‹", "æ•°æ®é‡‡é›†", "æŒ‡æ ‡é‡‡é›†"]
        
        for i, process in enumerate(processes):
            if process:
                name = process_names[i] if i < len(process_names) else f"è¿›ç¨‹{i}"
                self.logger.info(f"åœæ­¢{name}...")
                try:
                    process.terminate()
                    process.wait(timeout=30)
                    self.logger.info(f"{name}å·²åœæ­¢")
                except subprocess.TimeoutExpired:
                    self.logger.warning(f"{name}æœªå“åº”ï¼Œå¼ºåˆ¶ç»“æŸ")
                    try:
                        process.kill()
                        self.logger.info(f"{name}å·²å¼ºåˆ¶åœæ­¢")
                    except:
                        self.logger.error(f"æ— æ³•åœæ­¢{name}")
                except Exception as e:
                    self.logger.error(f"åœæ­¢{name}æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•° - ä¸€é”®å¯åŠ¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Train Ticket å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["monitor", "analyze"], default="monitor",
                       help="è¿è¡Œæ¨¡å¼: monitor=å®æ—¶ç›‘æ§, analyze=åˆ†æJSONç»“æœ")
    parser.add_argument("--date", type=str, help="æŒ‡å®šåˆ†ææ—¥æœŸ (YYYY-MM-DD)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Train Ticket Anomaly Detection System")
    print("=" * 60)
    
    try:
        controller = TrainTicketAnomalyDetectionController()
        
        if args.mode == "monitor":
            # åŸæœ‰çš„å®æ—¶ç›‘æ§æ¨¡å¼
            controller.run_real_time_monitoring()
        elif args.mode == "analyze":
            # æ–°çš„JSONåˆ†ææ¨¡å¼
            controller.run_json_analysis_mode()
        
        return 0
        
    except KeyboardInterrupt:
        print("\nSystem stopped by user")
        return 0
    except Exception as e:
        print(f"System execution failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

