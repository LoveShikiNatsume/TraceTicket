#!/usr/bin/env python3
"""
å°†å¼‚å¸¸æ•°æ®è½¬æ¢ä¸ºCSVæ ¼å¼
åŸºäºä¹‹å‰ç”Ÿæˆçš„å¼‚å¸¸æ•°æ®ï¼Œè½¬æ¢ä¸ºä¸åŸå§‹æ—¶é—´åç§»ä»£ç ç›¸åŒæ ¼å¼çš„è¾“å‡º
"""

import json
import pandas as pd
import numpy as np
import random
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import os

def load_anomaly_data(anomaly_data_dir):
    """åŠ è½½æ‰€æœ‰å¼‚å¸¸æ•°æ®"""
    print("ğŸ“‚ åŠ è½½å¼‚å¸¸æ•°æ®...")
    
    all_spans = []
    
    # éå†æ‰€æœ‰æ—¶é—´æ®µç›®å½•
    for period_dir in os.listdir(anomaly_data_dir):
        period_path = Path(anomaly_data_dir) / period_dir
        
        if not period_path.is_dir() or not period_dir.startswith('TT.'):
            continue
            
        time_period = period_dir.replace('TT.', '')
        print(f"  å¤„ç†æ—¶é—´æ®µ: {time_period}")
        
        # åŠ è½½æ‰€æœ‰ç±»å‹çš„æ•°æ®æ–‡ä»¶
        data_files = [
            'all_spans_with_anomalies.json'  # ä½¿ç”¨æ··åˆçš„æ•°æ®æ–‡ä»¶
        ]
        
        for data_file in data_files:
            file_path = period_path / data_file
            if not file_path.exists():
                continue
                
            try:
                with open(file_path, 'r') as f:
                    traces_data = json.load(f)
                
                # å¤„ç†æ¯ä¸ªtrace
                for trace in traces_data:
                    trace_id = trace['traceID']
                    processes = trace['processes']
                    anomaly_type = trace.get('anomaly_type', 'normal')
                    
                    for span in trace['spans']:
                        # è·å–åŸºæœ¬ä¿¡æ¯
                        process_id = span['processID']
                        service_name = processes[process_id]['serviceName']
                        operation_name = span['operationName']
                        
                        # è·å–çŠ¶æ€ç 
                        status_code = '200'
                        for tag in span.get('tags', []):
                            if tag['key'] in ['http.status_code', 'error']:
                                if tag['key'] == 'error' and tag['value']:
                                    status_code = 'error'
                                elif tag['key'] == 'http.status_code':
                                    status_code = str(tag['value'])
                                break
                        
                        # æ ¹æ®å¼‚å¸¸ç±»å‹è®¾ç½®æ ‡ç­¾
                        labels = {
                            'nodeLatencyLabel': 0,
                            'graphLatencyLabel': 0,
                            'graphStructureLabel': 0
                        }
                        
                        if anomaly_type == 'structure':
                            labels['graphStructureLabel'] = 1
                        elif anomaly_type == 'time':
                            labels['nodeLatencyLabel'] = 1
                            labels['graphLatencyLabel'] = 1
                        # normal ç±»å‹ä¿æŒå…¨0
                        
                        # åˆ›å»ºspanè®°å½•
                        span_record = {
                            'traceID': trace_id,
                            'spanID': span['spanID'],
                            'parentSpanID': span.get('parentSpanID', ''),
                            'operationName': operation_name,
                            'serviceName': service_name,
                            'startTime': span['startTime'],
                            'duration': span['duration'],
                            'status': status_code,
                            'time_period': time_period,
                            'anomaly_type': anomaly_type,
                            **labels
                        }
                        
                        all_spans.append(span_record)
                        
            except Exception as e:
                print(f"    âŒ å¤„ç†æ–‡ä»¶ {data_file} å¤±è´¥: {e}")
    
    print(f"âœ… æ€»è®¡åŠ è½½ {len(all_spans)} ä¸ªspans")
    return all_spans

def convert_to_csv_with_labels(spans, output_dir):
    """è½¬æ¢ä¸ºCSVæ ¼å¼å¹¶ä¿å­˜"""
    print("ğŸ“Š è½¬æ¢ä¸ºCSVæ ¼å¼...")
    
    # ç”ŸæˆIDæ˜ å°„
    operation_names = set(s['operationName'] for s in spans)
    service_names = set(s['serviceName'] for s in spans)
    
    operation_name_to_id = {op: i+1 for i, op in enumerate(sorted(operation_names))}
    service_name_to_id = {svc: i+1 for i, svc in enumerate(sorted(service_names))}
    
    # è½¬æ¢æ•°æ®
    csv_records = []
    trace_id_mapping = {}
    span_id_mapping = {}
    
    for span in spans:
        # IDæ˜ å°„
        if span['traceID'] not in trace_id_mapping:
            hash_obj = hashlib.md5(str(span['traceID']).encode())
            hash_int = int(hash_obj.hexdigest()[:16], 16)
            trace_id_high = (hash_int >> 32) & 0x7FFFFFFFFFFFFFFF
            trace_id_low = hash_int & 0x7FFFFFFFFFFFFFFF
            trace_id_mapping[span['traceID']] = (trace_id_high, trace_id_low)
        
        if span['spanID'] not in span_id_mapping:
            span_id_mapping[span['spanID']] = random.randint(-2**31, 2**31-1)
        
        trace_id_high, trace_id_low = trace_id_mapping[span['traceID']]
        span_id = span_id_mapping[span['spanID']]
        
        # å¤„ç†çˆ¶span ID
        parent_span_id = 0
        if span['parentSpanID']:
            if span['parentSpanID'] not in span_id_mapping:
                span_id_mapping[span['parentSpanID']] = random.randint(-2**31, 2**31-1)
            parent_span_id = span_id_mapping[span['parentSpanID']]
        
        # æ—¶é—´è½¬æ¢
        start_time_sec = span['startTime'] / 1000000
        start_time_str = datetime.fromtimestamp(start_time_sec).strftime("%Y-%m-%d %H:%M:%S")
        duration_ms = max(1, int(span['duration'] / 1000))
        nanosecond = (span['startTime'] % 1000000) * 1000
        
        # çŠ¶æ€å¤„ç†
        status_val = 0 if span['status'] == '200' else 1
        
        # CSVè®°å½•
        csv_record = {
            'traceIdHigh': trace_id_high,
            'traceIdLow': trace_id_low,
            'parentSpanId': parent_span_id,
            'spanId': span_id,
            'startTime': start_time_str,
            'duration': duration_ms,
            'nanosecond': int(nanosecond),
            'DBhash': 0,
            'status': status_val,
            'operationName': operation_name_to_id.get(span['operationName'], 0),
            'serviceName': service_name_to_id.get(span['serviceName'], 0),
            'nodeLatencyLabel': span['nodeLatencyLabel'],
            'graphLatencyLabel': span['graphLatencyLabel'],
            'graphStructureLabel': span['graphStructureLabel']
        }
        
        csv_records.append(csv_record)
    
    df = pd.DataFrame(csv_records)
    
    # åˆ†å‰²æ•°æ®é›†
    print("ğŸ”„ åˆ†å‰²æ•°æ®é›†...")
    
    # æŒ‰traceåˆ†ç»„
    trace_groups = df.groupby(['traceIdHigh', 'traceIdLow'])
    
    # åˆ†ç¦»æ­£å¸¸å’Œå¼‚å¸¸traces
    normal_traces = []
    anomaly_traces = []
    
    for trace_id, group in trace_groups:
        has_anomaly = (group['nodeLatencyLabel'].sum() > 0 or 
                      group['graphLatencyLabel'].sum() > 0 or 
                      group['graphStructureLabel'].sum() > 0)
        
        if has_anomaly:
            anomaly_traces.append(trace_id)
        else:
            normal_traces.append(trace_id)
    
    print(f"æ­£å¸¸traces: {len(normal_traces)}, å¼‚å¸¸traces: {len(anomaly_traces)}")
    
    # åˆ†åˆ«åˆ†å‰²
    random.shuffle(normal_traces)
    random.shuffle(anomaly_traces)
    
    # æ­£å¸¸tracesåˆ†å‰² (70%, 15%, 15%)
    normal_total = len(normal_traces)
    normal_train_size = int(0.7 * normal_total)
    normal_val_size = int(0.15 * normal_total)
    
    normal_train = normal_traces[:normal_train_size]
    normal_val = normal_traces[normal_train_size:normal_train_size + normal_val_size]
    normal_test = normal_traces[normal_train_size + normal_val_size:]
    
    # å¼‚å¸¸tracesåˆ†å‰²
    anomaly_total = len(anomaly_traces)
    anomaly_train_size = int(0.7 * anomaly_total)
    anomaly_val_size = int(0.15 * anomaly_total)
    
    anomaly_train = anomaly_traces[:anomaly_train_size]
    anomaly_val = anomaly_traces[anomaly_train_size:anomaly_train_size + anomaly_val_size]
    anomaly_test = anomaly_traces[anomaly_train_size + anomaly_val_size:]
    
    # åˆå¹¶
    train_traces = normal_train + anomaly_train
    val_traces = normal_val + anomaly_val
    test_traces = normal_test + anomaly_test
    
    # åˆ›å»ºæ•°æ®æ¡†
    train_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(train_traces)].reset_index(drop=True)
    val_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(val_traces)].reset_index(drop=True)
    test_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(test_traces)].reset_index(drop=True)
    
    # ä¿å­˜æ–‡ä»¶
    Path(output_dir).mkdir(exist_ok=True)

    # è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ é™¤å¼‚å¸¸æ ‡ç­¾åˆ—
    train_df_clean = train_df.drop(columns=['nodeLatencyLabel', 'graphLatencyLabel', 'graphStructureLabel'])
    val_df_clean = val_df.drop(columns=['nodeLatencyLabel', 'graphLatencyLabel', 'graphStructureLabel'])

    train_df_clean.to_csv(Path(output_dir) / "train.csv", index=False)
    val_df_clean.to_csv(Path(output_dir) / "val.csv", index=False)
    test_df.to_csv(Path(output_dir) / "test.csv", index=False)  # æµ‹è¯•é›†ä¿ç•™æ ‡ç­¾åˆ—
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("âœ… å¼‚å¸¸æ•°æ®è½¬æ¢å®Œæˆ!")
    
    # ç»Ÿè®¡å¼‚å¸¸ç±»å‹åˆ†å¸ƒ
    print("\nğŸ“Š å¼‚å¸¸ç±»å‹ç»Ÿè®¡:")
    anomaly_stats = defaultdict(int)
    for span in spans:
        anomaly_stats[span['anomaly_type']] += 1
    
    for anomaly_type, count in anomaly_stats.items():
        print(f"  {anomaly_type}: {count} spans")
    
    # ç»Ÿè®¡å„æ•°æ®é›†
    for split_name, split_df in [("è®­ç»ƒ", train_df), ("éªŒè¯", val_df), ("æµ‹è¯•", test_df)]:
        node_anomalies = split_df['nodeLatencyLabel'].sum()
        graph_lat_anomalies = split_df['graphLatencyLabel'].sum()
        graph_struct_anomalies = split_df['graphStructureLabel'].sum()
        total_spans = len(split_df)
        
        max_anomalies = max(node_anomalies, graph_lat_anomalies, graph_struct_anomalies)
        anomaly_ratio = max_anomalies / total_spans * 100 if total_spans > 0 else 0
        
        print(f"{split_name}é›†: {total_spans} spans, å¼‚å¸¸ç‡: {anomaly_ratio:.2f}%")
        print(f"  - èŠ‚ç‚¹å»¶è¿Ÿ: {node_anomalies}, å›¾å»¶è¿Ÿ: {graph_lat_anomalies}, å›¾ç»“æ„: {graph_struct_anomalies}")
    
    return operation_name_to_id, service_name_to_id

def save_yaml_files(operation_name_to_id, service_name_to_id, output_dir):
    """ä¿å­˜YAMLé…ç½®æ–‡ä»¶"""
    print("ğŸ“ ä¿å­˜YAMLé…ç½®æ–‡ä»¶...")
    
    id_manager_dir = Path(output_dir) / "id_manager"
    id_manager_dir.mkdir(parents=True, exist_ok=True)
    
    # operation_id.yml
    operation_file = id_manager_dir / "operation_id.yml"
    with open(operation_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        composite_key = f"{service_id}/{operation_id}"
                        f.write(f"{composite_key}: {global_id}\n")
                        global_id += 1
    
    # service_id.yml
    service_file = id_manager_dir / "service_id.yml"
    with open(service_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        for svc_id in sorted(service_name_to_id.values()):
            if svc_id != 0:
                f.write(f"'{svc_id}': {svc_id}\n")
    
    # status_id.yml
    status_file = id_manager_dir / "status_id.yml"
    with open(status_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        f.write("'0': 1\n")
        f.write("'1': 2\n")
        f.write("'200': 3\n")
    
    # latency_range.yml (ç®€åŒ–ç‰ˆæœ¬)
    latency_file = id_manager_dir / "latency_range.yml"
    with open(latency_file, 'w', encoding='utf-8') as f:
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        f.write(f"{global_id}:\n")
                        f.write(f"  mean: 10.0\n")
                        f.write(f"  std: 5.0\n")
                        f.write(f"  p99: 50.0\n")
                        global_id += 1
    
    print(f"âœ… YAMLæ–‡ä»¶å·²ä¿å­˜åˆ°: {id_manager_dir}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    anomaly_data_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/anomaly_data"  # ä¹‹å‰ç”Ÿæˆçš„å¼‚å¸¸æ•°æ®ç›®å½•
    output_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/convert_structure_anomaly_data"  # è¾“å‡ºCSVç›®å½•
    
    print("ğŸ”„ å¼€å§‹è½¬æ¢å¼‚å¸¸æ•°æ®ä¸ºCSVæ ¼å¼...")
    
    try:
        # 1. åŠ è½½å¼‚å¸¸æ•°æ®
        spans = load_anomaly_data(anomaly_data_dir)
        
        if not spans:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•spansæ•°æ®")
            return
        
        # 2. è½¬æ¢ä¸ºCSVå¹¶ä¿å­˜
        operation_name_to_id, service_name_to_id = convert_to_csv_with_labels(spans, output_dir)
        
        # 3. ä¿å­˜YAMLæ–‡ä»¶
        save_yaml_files(operation_name_to_id, service_name_to_id, output_dir)
        
        print(f"\nğŸ¯ CSVæ•°æ®é›†å·²ä¿å­˜åˆ°: {output_dir}")
        print("ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  â”œâ”€â”€ train.csv  (è®­ç»ƒé›†ï¼Œæ— æ ‡ç­¾åˆ—)")
        print(f"  â”œâ”€â”€ val.csv    (éªŒè¯é›†ï¼Œæ— æ ‡ç­¾åˆ—)")
        print(f"  â”œâ”€â”€ test.csv   (æµ‹è¯•é›†ï¼Œå«æ ‡ç­¾åˆ—)")
        print(f"  â””â”€â”€ id_manager/ (YAMLé…ç½®æ–‡ä»¶)")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"  bash test.sh results/train/models/final.pt {output_dir}")
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    random.seed(42)
    main()