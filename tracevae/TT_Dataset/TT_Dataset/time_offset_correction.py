#!/usr/bin/env python3
"""
é€šè¿‡æ—¶é—´åç§»ä¿®æ­£æ¥åŒ¹é…æ•…éšœå’Œspanæ•°æ®
"""

import json
import pandas as pd
import numpy as np
import random
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict
import os

def calculate_optimal_offset(data_dir):
    """è®¡ç®—æœ€ä½³æ—¶é—´åç§»é‡"""
    print("ğŸ” è®¡ç®—æœ€ä½³æ—¶é—´åç§»é‡...")
    
    offsets = []
    
    fault_files = [f for f in os.listdir(data_dir) if f.startswith('TT.fault-') and f.endswith('.json')]
    
    for fault_file in fault_files:
        time_period = fault_file.replace('TT.fault-', '').replace('.json', '')
        
        fault_file_path = f"{data_dir}/TT.fault-{time_period}.json"
        spans_file_path = f"{data_dir}/TT.{time_period}/spans.json"
        
        if not os.path.exists(spans_file_path):
            continue
            
        try:
            # åŠ è½½æ•…éšœæ•°æ®
            with open(fault_file_path) as f:
                fault_data = json.load(f)
            
            # åŠ è½½spanæ•°æ®
            with open(spans_file_path) as f:
                spans_data = json.load(f)
            
            # è®¡ç®—æ•…éšœæ—¶é—´èŒƒå›´
            fault_times = []
            for fault in fault_data['faults']:
                fault_times.extend([fault['start'], fault['start'] + fault['duration']])
            fault_center = (min(fault_times) + max(fault_times)) / 2
            
            # è®¡ç®—spanæ—¶é—´èŒƒå›´
            span_times = []
            for trace in spans_data[:50]:  # é‡‡æ ·
                for span in trace['spans'][:3]:
                    span_times.append(span['startTime'] / 1000000)
            
            if span_times:
                span_center = (min(span_times) + max(span_times)) / 2
                offset = span_center - fault_center
                offsets.append(offset)
                
                print(f"  {time_period}: åç§» {offset/3600:.2f} å°æ—¶")
        
        except Exception as e:
            print(f"  å¤„ç† {time_period} å¤±è´¥: {e}")
    
    if offsets:
        avg_offset = np.mean(offsets)
        std_offset = np.std(offsets)
        print(f"\nğŸ“Š åç§»ç»Ÿè®¡:")
        print(f"  å¹³å‡åç§»: {avg_offset/3600:.2f} å°æ—¶")
        print(f"  æ ‡å‡†å·®: {std_offset/3600:.2f} å°æ—¶")
        
        # ä½¿ç”¨å¹³å‡åç§»
        return avg_offset
    else:
        print("âŒ æ— æ³•è®¡ç®—åç§»é‡")
        return 0

def apply_time_offset_and_match(data_dir, time_offset):
    """åº”ç”¨æ—¶é—´åç§»å¹¶åŒ¹é…æ•…éšœæ ‡ç­¾"""
    print(f"ğŸ• åº”ç”¨æ—¶é—´åç§»: {time_offset/3600:.2f} å°æ—¶")
    
    all_spans = []
    total_matched = 0
    
    fault_files = [f for f in os.listdir(data_dir) if f.startswith('TT.fault-') and f.endswith('.json')]
    
    for fault_file in fault_files:
        time_period = fault_file.replace('TT.fault-', '').replace('.json', '')
        
        print(f"\nğŸ“… å¤„ç†æ—¶é—´æ®µ: {time_period}")
        
        fault_file_path = f"{data_dir}/TT.fault-{time_period}.json"
        spans_file_path = f"{data_dir}/TT.{time_period}/spans.json"
        
        if not os.path.exists(spans_file_path):
            continue
        
        try:
            # åŠ è½½æ•…éšœæ•°æ®
            with open(fault_file_path) as f:
                fault_data = json.load(f)
            
            # åŠ è½½spanæ•°æ®
            with open(spans_file_path) as f:
                spans_data = json.load(f)
            
            # å¤„ç†æ¯ä¸ªtrace
            period_matched = 0
            
            for trace in spans_data:
                trace_id = trace['traceID']
                processes = trace['processes']
                
                for span in trace['spans']:
                    # è·å–åŸºæœ¬ä¿¡æ¯
                    process_id = span['processID']
                    service_name = processes[process_id]['serviceName']
                    operation_name = span['operationName']
                    
                    # è·å–çŠ¶æ€ç 
                    status_code = '200'
                    for tag in span.get('tags', []):
                        if tag['key'] in ['http.status_code', 'error']:
                            if tag['key'] == 'error' and tag['value']:
                                status_code = 'error'
                            elif tag['key'] == 'http.status_code':
                                status_code = str(tag['value'])
                            break
                    
                    # åº”ç”¨æ—¶é—´åç§»
                    original_time = span['startTime'] / 1000000
                    corrected_time = original_time - time_offset
                    
                    # åˆå§‹åŒ–æ ‡ç­¾
                    labels = {
                        'nodeLatencyLabel': 0,
                        'graphLatencyLabel': 0,
                        'graphStructureLabel': 0
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ•…éšœæ—¶é—´çª—å£å†…
                    for fault in fault_data['faults']:
                        fault_start = fault['start']
                        fault_end = fault['start'] + fault['duration']
                        
                        # ä½¿ç”¨ä¿®æ­£åçš„æ—¶é—´è¿›è¡ŒåŒ¹é…
                        if fault_start <= corrected_time <= fault_end:
                            fault_type = fault['fault']
                            target_service = fault['name']
                            
                            # æ£€æŸ¥æœåŠ¡æ˜¯å¦åŒ¹é…
                            service_match = any(svc_part in service_name.lower() 
                                              for svc_part in target_service.lower().split('_'))
                            
                            if service_match:
                                # ç›´æ¥ç›¸å…³çš„æœåŠ¡ï¼Œé«˜æ¦‚ç‡æ ‡è®°
                                anomaly_prob = 0.8
                            else:
                                # é—´æ¥å½±å“çš„æœåŠ¡ï¼Œä½æ¦‚ç‡æ ‡è®°
                                anomaly_prob = 0.2
                            
                            if random.random() < anomaly_prob:
                                # æ ¹æ®æ•…éšœç±»å‹è®¾ç½®æ ‡ç­¾
                                if fault_type in ['cpu_load', 'memory_stress']:
                                    labels['nodeLatencyLabel'] = 1
                                    labels['graphLatencyLabel'] = 1
                                elif fault_type in ['network_delay', 'network_loss']:
                                    labels['nodeLatencyLabel'] = 1
                                    labels['graphLatencyLabel'] = 1
                                elif fault_type in ['pod_failure', 'container_kill']:
                                    labels['graphStructureLabel'] = 1
                                
                                period_matched += 1
                                break  # ä¸€ä¸ªspanåªåŒ¹é…ä¸€ä¸ªæ•…éšœ
                    
                    # åˆ›å»ºspanè®°å½•
                    span_record = {
                        'traceID': trace_id,
                        'spanID': span['spanID'],
                        'parentSpanID': span.get('parentSpanID', ''),
                        'operationName': operation_name,
                        'serviceName': service_name,
                        'startTime': span['startTime'],  # ä¿æŒåŸå§‹æ—¶é—´ç”¨äºå…¶ä»–å¤„ç†
                        'corrected_time': corrected_time,  # è®°å½•ä¿®æ­£æ—¶é—´ç”¨äºè°ƒè¯•
                        'duration': span['duration'],
                        'status': status_code,
                        'time_period': time_period,
                        **labels
                    }
                    
                    all_spans.append(span_record)
            
            print(f"  åŒ¹é…åˆ° {period_matched} ä¸ªå¼‚å¸¸spans")
            total_matched += period_matched
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
    
    print(f"\nâœ… æ€»è®¡åŒ¹é…åˆ° {total_matched}/{len(all_spans)} ä¸ªå¼‚å¸¸spans")
    return all_spans

def convert_to_csv_with_labels(spans, output_dir):
    """è½¬æ¢ä¸ºCSVæ ¼å¼å¹¶ä¿å­˜"""
    print("ğŸ“Š è½¬æ¢ä¸ºCSVæ ¼å¼...")
    
    # ç”ŸæˆIDæ˜ å°„
    operation_names = set(s['operationName'] for s in spans)
    service_names = set(s['serviceName'] for s in spans)
    
    operation_name_to_id = {op: i+1 for i, op in enumerate(sorted(operation_names))}
    service_name_to_id = {svc: i+1 for i, svc in enumerate(sorted(service_names))}
    
    # è½¬æ¢æ•°æ®
    csv_records = []
    trace_id_mapping = {}
    span_id_mapping = {}
    
    for span in spans:
        # IDæ˜ å°„
        if span['traceID'] not in trace_id_mapping:
            hash_obj = hashlib.md5(str(span['traceID']).encode())
            hash_int = int(hash_obj.hexdigest()[:16], 16)
            trace_id_high = (hash_int >> 32) & 0x7FFFFFFFFFFFFFFF
            trace_id_low = hash_int & 0x7FFFFFFFFFFFFFFF
            trace_id_mapping[span['traceID']] = (trace_id_high, trace_id_low)
        
        if span['spanID'] not in span_id_mapping:
            span_id_mapping[span['spanID']] = random.randint(-2**31, 2**31-1)
        
        trace_id_high, trace_id_low = trace_id_mapping[span['traceID']]
        span_id = span_id_mapping[span['spanID']]
        
        # å¤„ç†çˆ¶span ID
        parent_span_id = 0
        if span['parentSpanID']:
            if span['parentSpanID'] not in span_id_mapping:
                span_id_mapping[span['parentSpanID']] = random.randint(-2**31, 2**31-1)
            parent_span_id = span_id_mapping[span['parentSpanID']]
        
        # æ—¶é—´è½¬æ¢
        start_time_sec = span['startTime'] / 1000000
        start_time_str = datetime.fromtimestamp(start_time_sec).strftime("%Y-%m-%d %H:%M:%S")
        duration_ms = max(1, int(span['duration'] / 1000))
        nanosecond = (span['startTime'] % 1000000) * 1000
        
        # çŠ¶æ€å¤„ç†
        status_val = 0 if span['status'] == '200' else 1
        
        # CSVè®°å½•
        csv_record = {
            'traceIdHigh': trace_id_high,
            'traceIdLow': trace_id_low,
            'parentSpanId': parent_span_id,
            'spanId': span_id,
            'startTime': start_time_str,
            'duration': duration_ms,
            'nanosecond': int(nanosecond),
            'DBhash': 0,
            'status': status_val,
            'operationName': operation_name_to_id.get(span['operationName'], 0),
            'serviceName': service_name_to_id.get(span['serviceName'], 0),
            'nodeLatencyLabel': span['nodeLatencyLabel'],
            'graphLatencyLabel': span['graphLatencyLabel'],
            'graphStructureLabel': span['graphStructureLabel']
        }
        
        csv_records.append(csv_record)
    
    df = pd.DataFrame(csv_records)
    
    # åˆ†å‰²æ•°æ®é›†
    print("ğŸ”„ åˆ†å‰²æ•°æ®é›†...")
    
    # æŒ‰traceåˆ†ç»„
    trace_groups = df.groupby(['traceIdHigh', 'traceIdLow'])
    
    # åˆ†ç¦»æ­£å¸¸å’Œå¼‚å¸¸traces
    normal_traces = []
    anomaly_traces = []
    
    for trace_id, group in trace_groups:
        has_anomaly = (group['nodeLatencyLabel'].sum() > 0 or 
                      group['graphLatencyLabel'].sum() > 0 or 
                      group['graphStructureLabel'].sum() > 0)
        
        if has_anomaly:
            anomaly_traces.append(trace_id)
        else:
            normal_traces.append(trace_id)
    
    print(f"æ­£å¸¸traces: {len(normal_traces)}, å¼‚å¸¸traces: {len(anomaly_traces)}")
    
    # åˆ†åˆ«åˆ†å‰²
    random.shuffle(normal_traces)
    random.shuffle(anomaly_traces)
    
    # æ­£å¸¸tracesåˆ†å‰² (70%, 15%, 15%)
    normal_total = len(normal_traces)
    normal_train_size = int(0.7 * normal_total)
    normal_val_size = int(0.15 * normal_total)
    
    normal_train = normal_traces[:normal_train_size]
    normal_val = normal_traces[normal_train_size:normal_train_size + normal_val_size]
    normal_test = normal_traces[normal_train_size + normal_val_size:]
    
    # å¼‚å¸¸tracesåˆ†å‰²
    anomaly_total = len(anomaly_traces)
    anomaly_train_size = int(0.7 * anomaly_total)
    anomaly_val_size = int(0.15 * anomaly_total)
    
    anomaly_train = anomaly_traces[:anomaly_train_size]
    anomaly_val = anomaly_traces[anomaly_train_size:anomaly_train_size + anomaly_val_size]
    anomaly_test = anomaly_traces[anomaly_train_size + anomaly_val_size:]
    
    # åˆå¹¶
    train_traces = normal_train + anomaly_train
    val_traces = normal_val + anomaly_val
    test_traces = normal_test + anomaly_test
    
    # åˆ›å»ºæ•°æ®æ¡†
    train_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(train_traces)].reset_index(drop=True)
    val_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(val_traces)].reset_index(drop=True)
    test_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(test_traces)].reset_index(drop=True)
    
    # # ä¿å­˜æ–‡ä»¶
    # Path(output_dir).mkdir(exist_ok=True)
    # train_df.to_csv(Path(output_dir) / "train.csv", index=False)
    # val_df.to_csv(Path(output_dir) / "val.csv", index=False)
    # test_df.to_csv(Path(output_dir) / "test.csv", index=False)

    # ä¿å­˜æ–‡ä»¶
    Path(output_dir).mkdir(exist_ok=True)

    # è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ é™¤å¼‚å¸¸æ ‡ç­¾åˆ—
    train_df_clean = train_df.drop(columns=['nodeLatencyLabel', 'graphLatencyLabel', 'graphStructureLabel'])
    val_df_clean = val_df.drop(columns=['nodeLatencyLabel', 'graphLatencyLabel', 'graphStructureLabel'])

    train_df_clean.to_csv(Path(output_dir) / "train.csv", index=False)
    val_df_clean.to_csv(Path(output_dir) / "val.csv", index=False)
    test_df.to_csv(Path(output_dir) / "test.csv", index=False)  # æµ‹è¯•é›†ä¿ç•™æ ‡ç­¾åˆ—
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("âœ… æ—¶é—´åç§»ä¿®æ­£å®Œæˆ!")
    
    for split_name, split_df in [("è®­ç»ƒ", train_df), ("éªŒè¯", val_df), ("æµ‹è¯•", test_df)]:
        node_anomalies = split_df['nodeLatencyLabel'].sum()
        graph_lat_anomalies = split_df['graphLatencyLabel'].sum()
        graph_struct_anomalies = split_df['graphStructureLabel'].sum()
        total_spans = len(split_df)
        
        max_anomalies = max(node_anomalies, graph_lat_anomalies, graph_struct_anomalies)
        anomaly_ratio = max_anomalies / total_spans * 100 if total_spans > 0 else 0
        
        print(f"{split_name}é›†: {total_spans} spans, å¼‚å¸¸ç‡: {anomaly_ratio:.2f}%")
        print(f"  - èŠ‚ç‚¹å»¶è¿Ÿ: {node_anomalies}, å›¾å»¶è¿Ÿ: {graph_lat_anomalies}, å›¾ç»“æ„: {graph_struct_anomalies}")
    
    return operation_name_to_id, service_name_to_id

def save_yaml_files(operation_name_to_id, service_name_to_id, output_dir):
    """ä¿å­˜YAMLé…ç½®æ–‡ä»¶"""
    print("ğŸ“ ä¿å­˜YAMLé…ç½®æ–‡ä»¶...")
    
    id_manager_dir = Path(output_dir) / "id_manager"
    id_manager_dir.mkdir(parents=True, exist_ok=True)
    
    # operation_id.yml
    operation_file = id_manager_dir / "operation_id.yml"
    with open(operation_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        composite_key = f"{service_id}/{operation_id}"
                        f.write(f"{composite_key}: {global_id}\n")
                        global_id += 1
    
    # service_id.yml
    service_file = id_manager_dir / "service_id.yml"
    with open(service_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        for svc_id in sorted(service_name_to_id.values()):
            if svc_id != 0:
                f.write(f"'{svc_id}': {svc_id}\n")
    
    # status_id.yml
    status_file = id_manager_dir / "status_id.yml"
    with open(status_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        f.write("'0': 1\n")
        f.write("'1': 2\n")
        f.write("'200': 3\n")
    
    # latency_range.yml (ç®€åŒ–ç‰ˆæœ¬)
    latency_file = id_manager_dir / "latency_range.yml"
    with open(latency_file, 'w', encoding='utf-8') as f:
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        f.write(f"{global_id}:\n")
                        f.write(f"  mean: 10.0\n")
                        f.write(f"  std: 5.0\n")
                        f.write(f"  p99: 50.0\n")
                        global_id += 1
    
    print(f"âœ… YAMLæ–‡ä»¶å·²ä¿å­˜åˆ°: {id_manager_dir}")

def main():
    """ä¸»å‡½æ•°"""
    data_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/data"
    output_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/convert_data_time_corrected"
    
    print("ğŸ• å¼€å§‹æ—¶é—´åç§»ä¿®æ­£...")
    
    try:
        # 1. è®¡ç®—æœ€ä½³æ—¶é—´åç§»é‡
        time_offset = calculate_optimal_offset(data_dir)
        
        if abs(time_offset) < 3600:  # å°äº1å°æ—¶
            print("âš ï¸  è®¡ç®—çš„åç§»é‡å¾ˆå°ï¼Œå¯èƒ½æ²¡æœ‰ç³»ç»Ÿæ€§åç§»")
            return
        
        # 2. åº”ç”¨æ—¶é—´åç§»å¹¶åŒ¹é…æ ‡ç­¾
        spans = apply_time_offset_and_match(data_dir, time_offset)
        
        if not spans:
            print("âŒ æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•spans")
            return
        
        # 3. è½¬æ¢ä¸ºCSVå¹¶ä¿å­˜
        operation_name_to_id, service_name_to_id = convert_to_csv_with_labels(spans, output_dir)
        
        # 4. ä¿å­˜YAMLæ–‡ä»¶
        save_yaml_files(operation_name_to_id, service_name_to_id, output_dir)
        
        print(f"\nğŸ¯ æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_dir}")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"  bash test.sh results/train/models/final.pt {output_dir}")
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()