#!/usr/bin/env python3
"""
ä¿®å¤çš„TTæ•°æ®é›†è½¬æ¢è„šæœ¬ - æ­£ç¡®ä½¿ç”¨æ•…éšœæ³¨å…¥æ•°æ®
"""

import json
import pandas as pd
import yaml
import random
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import numpy as np
import os

def load_fault_injection_data(data_dir):
    """åŠ è½½æ‰€æœ‰æ•…éšœæ³¨å…¥æ•°æ®"""
    print("åŠ è½½æ•…éšœæ³¨å…¥æ•°æ®...")
    
    fault_data = {}
    fault_files = [f for f in os.listdir(data_dir) if f.startswith('TT.fault-') and f.endswith('.json')]
    
    for fault_file in fault_files:
        print(f"  - åŠ è½½: {fault_file}")
        with open(os.path.join(data_dir, fault_file)) as f:
            fault_info = json.load(f)
        
        # æå–æ—¶é—´æ®µæ ‡è¯†
        time_period = fault_file.replace('TT.fault-', '').replace('.json', '')
        fault_data[time_period] = fault_info
        
        print(f"    å®éªŒå¼€å§‹: {datetime.fromtimestamp(fault_info['start'])}")
        print(f"    æ•…éšœæ•°é‡: {len(fault_info['faults'])}")
        
        for fault in fault_info['faults']:
            fault_start = datetime.fromtimestamp(fault['start'])
            fault_end = datetime.fromtimestamp(fault['start'] + fault['duration'])
            print(f"      {fault['fault']} on {fault['name']}: {fault_start} - {fault_end}")
    
    return fault_data

def load_json_traces_with_time_period(data_dir):
    """åŠ è½½æ‰€æœ‰æ—¶é—´æ®µçš„traceæ•°æ®"""
    print("åŠ è½½traceæ•°æ®...")
    
    all_traces = []
    spans_files = []
    
    # æŸ¥æ‰¾æ‰€æœ‰spans.jsonæ–‡ä»¶
    for item in os.listdir(data_dir):
        item_path = os.path.join(data_dir, item)
        if os.path.isdir(item_path):
            spans_file = os.path.join(item_path, 'spans.json')
            if os.path.exists(spans_file):
                spans_files.append((item, spans_file))
    
    for time_period, spans_file in spans_files:
        print(f"  - åŠ è½½: {time_period}/spans.json")
        with open(spans_file) as f:
            traces = json.load(f)
        
        # ä¸ºæ¯ä¸ªtraceæ·»åŠ æ—¶é—´æ®µæ ‡è¯†
        for trace in traces:
            trace['time_period'] = time_period
        
        all_traces.extend(traces)
        print(f"    traceæ•°é‡: {len(traces)}")
    
    print(f"æ€»traceæ•°é‡: {len(all_traces)}")
    return all_traces

def match_spans_with_faults(spans, fault_data):
    """å°†spansä¸æ•…éšœæ•°æ®åŒ¹é…ï¼Œç”ŸæˆçœŸå®æ ‡ç­¾"""
    print("åŒ¹é…spansä¸æ•…éšœæ•°æ®...")
    
    labeled_spans = []
    
    for span in spans:
        time_period = span.get('time_period', '')
        
        # é»˜è®¤æ ‡ç­¾
        labels = {
            'nodeLatencyLabel': 0,
            'graphLatencyLabel': 0, 
            'graphStructureLabel': 0,
            'is_fault_period': 0,
            'fault_types': []
        }
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ•…éšœæ—¶é—´æ®µå†…
        if time_period in fault_data:
            fault_info = fault_data[time_period]
            span_start_time = span['startTime'] / 1000000  # å¾®ç§’è½¬ç§’
            
            labels['is_fault_period'] = 1  # æ ‡è®°ä¸ºæ•…éšœå®éªŒæœŸé—´
            
            # æ£€æŸ¥æ¯ä¸ªæ•…éšœ
            for fault in fault_info['faults']:
                fault_start = fault['start']
                fault_end = fault['start'] + fault['duration']
                
                # æ£€æŸ¥spanæ˜¯å¦åœ¨æ•…éšœæ—¶é—´çª—å£å†…
                if fault_start <= span_start_time <= fault_end:
                    fault_type = fault['fault']
                    labels['fault_types'].append(fault_type)
                    
                    # æ ¹æ®æ•…éšœç±»å‹è®¾ç½®æ ‡ç­¾
                    if fault_type in ['cpu_load', 'memory_stress']:
                        labels['nodeLatencyLabel'] = 1
                        labels['graphLatencyLabel'] = 1
                    elif fault_type in ['network_delay', 'network_loss']:
                        labels['nodeLatencyLabel'] = 1
                        labels['graphLatencyLabel'] = 1
                    elif fault_type in ['pod_failure', 'container_kill']:
                        labels['graphStructureLabel'] = 1
                    else:
                        # æœªçŸ¥æ•…éšœç±»å‹ï¼Œæ ‡è®°ä¸ºå»¶è¿Ÿå¼‚å¸¸
                        labels['nodeLatencyLabel'] = 1
                        labels['graphLatencyLabel'] = 1
        
        # åˆå¹¶spanæ•°æ®å’Œæ ‡ç­¾
        span_with_labels = {**span, **labels}
        labeled_spans.append(span_with_labels)
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    node_anomalies = sum(1 for s in labeled_spans if s['nodeLatencyLabel'] == 1)
    graph_lat_anomalies = sum(1 for s in labeled_spans if s['graphLatencyLabel'] == 1)
    graph_struct_anomalies = sum(1 for s in labeled_spans if s['graphStructureLabel'] == 1)
    fault_period_spans = sum(1 for s in labeled_spans if s['is_fault_period'] == 1)
    
    print(f"æ ‡ç­¾ç»Ÿè®¡:")
    print(f"  - æ•…éšœæœŸé—´spans: {fault_period_spans}")
    print(f"  - èŠ‚ç‚¹å»¶è¿Ÿå¼‚å¸¸: {node_anomalies}")
    print(f"  - å›¾å»¶è¿Ÿå¼‚å¸¸: {graph_lat_anomalies}")
    print(f"  - å›¾ç»“æ„å¼‚å¸¸: {graph_struct_anomalies}")
    print(f"  - æ­£å¸¸spans: {len(labeled_spans) - max(node_anomalies, graph_lat_anomalies, graph_struct_anomalies)}")
    
    return labeled_spans

def extract_spans_from_labeled_traces(labeled_traces):
    """ä»å¸¦æ ‡ç­¾çš„tracesä¸­æå–spans"""
    print("æå–spans...")
    
    all_spans = []
    operation_names = set()
    service_names = set()
    status_codes = set()
    
    for trace in labeled_traces:
        trace_id = trace['traceID']
        processes = trace['processes']
        time_period = trace.get('time_period', '')
        
        for span in trace['spans']:
            # è·å–æœåŠ¡å
            process_id = span['processID']
            service_name = processes[process_id]['serviceName']
            
            # è·å–æ“ä½œå
            operation_name = span['operationName']
            
            # è·å–çŠ¶æ€ç 
            status_code = '200'
            for tag in span.get('tags', []):
                if tag['key'] in ['http.status_code', 'error']:
                    if tag['key'] == 'error' and tag['value']:
                        status_code = 'error'
                    elif tag['key'] == 'http.status_code':
                        status_code = str(tag['value'])
                    break
            
            # åˆ›å»ºspanè®°å½•ï¼ˆåŒ…å«æ ‡ç­¾ä¿¡æ¯ï¼‰
            span_record = {
                'traceID': trace_id,
                'spanID': span['spanID'],
                'parentSpanID': span.get('parentSpanID', ''),
                'operationName': operation_name,
                'serviceName': service_name,
                'startTime': span['startTime'],
                'duration': span['duration'],
                'status': status_code,
                'time_period': time_period,
                # æ•…éšœæ ‡ç­¾ä¿¡æ¯éœ€è¦ä»traceçº§åˆ«ä¼ é€’åˆ°spançº§åˆ«
                'nodeLatencyLabel': 0,  # ç¨åè®¾ç½®
                'graphLatencyLabel': 0,
                'graphStructureLabel': 0
            }
            
            all_spans.append(span_record)
            operation_names.add(operation_name)
            service_names.add(service_name)
            status_codes.add(status_code)
    
    return all_spans, operation_names, service_names, status_codes

def apply_fault_labels_to_spans(spans, fault_data):
    """ä¸ºspansåº”ç”¨æ•…éšœæ ‡ç­¾"""
    print("åº”ç”¨æ•…éšœæ ‡ç­¾åˆ°spans...")
    
    for span in spans:
        time_period = span['time_period']
        
        if time_period in fault_data:
            fault_info = fault_data[time_period]
            span_start_time = span['startTime'] / 1000000
            
            # æ£€æŸ¥æ¯ä¸ªæ•…éšœ
            for fault in fault_info['faults']:
                fault_start = fault['start']
                fault_end = fault['start'] + fault['duration']
                
                if fault_start <= span_start_time <= fault_end:
                    fault_type = fault['fault']
                    
                    if fault_type in ['cpu_load', 'memory_stress', 'network_delay', 'network_loss']:
                        span['nodeLatencyLabel'] = 1
                        span['graphLatencyLabel'] = 1
                    elif fault_type in ['pod_failure', 'container_kill']:
                        span['graphStructureLabel'] = 1
    
    return spans

# ä¿æŒå…¶ä»–å‡½æ•°ä¸å˜ï¼Œä½†ä¿®æ”¹mainå‡½æ•°
def main():
    """ä¿®å¤çš„ä¸»å‡½æ•°"""
    data_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/data"
    output_dir = "/home/fuxian/tracevae/TT_Dataset/TT_Dataset/convert_data_fixed"
    
    Path(output_dir).mkdir(exist_ok=True)
    
    print("ğŸš€ å¼€å§‹ä¿®å¤çš„TTæ•°æ®é›†è½¬æ¢...")
    
    try:
        # 1. åŠ è½½æ•…éšœæ³¨å…¥æ•°æ®
        fault_data = load_fault_injection_data(data_dir)
        
        # 2. åŠ è½½æ‰€æœ‰traces
        traces = load_json_traces_with_time_period(data_dir)
        
        # 3. æå–spans
        spans, operations, services, statuses = extract_spans_from_labeled_traces(traces)
        
        # 4. åº”ç”¨çœŸå®çš„æ•…éšœæ ‡ç­¾
        spans = apply_fault_labels_to_spans(spans, fault_data)
        
        # 5. ç”ŸæˆIDæ˜ å°„ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        operation_name_to_id, service_name_to_id, status_name_to_id, operation_durations = generate_id_mappings(
            operations, services, statuses, spans
        )
        
        # 6. è½¬æ¢ä¸ºCSVæ ¼å¼
        df = convert_spans_to_csv_format_with_labels(spans, operation_name_to_id, service_name_to_id, status_name_to_id)
        
        # 7. åˆ†å‰²æ•°æ®é›†ï¼ˆç¡®ä¿ä¿ç•™æ ‡ç­¾ï¼‰
        train_df, val_df, test_df = split_dataset_preserve_labels(df)
        
        # 8. ä¿å­˜æ–‡ä»¶
        train_df.to_csv(Path(output_dir) / "train.csv", index=False)
        val_df.to_csv(Path(output_dir) / "val.csv", index=False)
        test_df.to_csv(Path(output_dir) / "test.csv", index=False)
        
        # 9. ä¿å­˜YAMLæ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        save_yaml_files(operation_name_to_id, service_name_to_id, status_name_to_id, operation_durations, output_dir)
        
        print("âœ… ä¿®å¤çš„è½¬æ¢å®Œæˆ!")
        print(f"è®­ç»ƒé›†: {len(train_df)} æ¡è®°å½•")
        print(f"éªŒè¯é›†: {len(val_df)} æ¡è®°å½•")
        print(f"æµ‹è¯•é›†: {len(test_df)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
        for split_name, split_df in [("è®­ç»ƒ", train_df), ("éªŒè¯", val_df), ("æµ‹è¯•", test_df)]:
            node_anomalies = split_df['nodeLatencyLabel'].sum()
            graph_lat_anomalies = split_df['graphLatencyLabel'].sum()
            graph_struct_anomalies = split_df['graphStructureLabel'].sum()
            print(f"{split_name}é›†å¼‚å¸¸åˆ†å¸ƒ: èŠ‚ç‚¹å»¶è¿Ÿ={node_anomalies}, å›¾å»¶è¿Ÿ={graph_lat_anomalies}, å›¾ç»“æ„={graph_struct_anomalies}")
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def convert_spans_to_csv_format_with_labels(spans, operation_name_to_id, service_name_to_id, status_name_to_id):
    """è½¬æ¢spansä¸ºCSVæ ¼å¼ï¼Œä¿ç•™æ ‡ç­¾ä¿¡æ¯"""
    print("è½¬æ¢spansä¸ºCSVæ ¼å¼ï¼ˆåŒ…å«çœŸå®æ ‡ç­¾ï¼‰...")
    
    csv_records = []
    trace_id_mapping = {}
    span_id_mapping = {}
    
    for span in spans:
        # ç”ŸæˆIDæ˜ å°„ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if span['traceID'] not in trace_id_mapping:
            trace_id_mapping[span['traceID']] = generate_realistic_ids(span['traceID'])
        
        if span['spanID'] not in span_id_mapping:
            span_id_mapping[span['spanID']] = generate_span_id()
        
        trace_id_high, trace_id_low = trace_id_mapping[span['traceID']]
        span_id = span_id_mapping[span['spanID']]
        
        # å¤„ç†çˆ¶span ID
        parent_span_id = 0
        if span['parentSpanID']:
            if span['parentSpanID'] not in span_id_mapping:
                span_id_mapping[span['parentSpanID']] = generate_span_id()
            parent_span_id = span_id_mapping[span['parentSpanID']]
        
        # è½¬æ¢æ—¶é—´
        start_time_sec = span['startTime'] / 1000000
        start_time_str = datetime.fromtimestamp(start_time_sec).strftime("%Y-%m-%d %H:%M:%S")
        nanosecond = (span['startTime'] % 1000000) * 1000
        
        # è½¬æ¢duration
        duration_ms = max(1, int(span['duration'] / 1000))
        
        # å¤„ç†çŠ¶æ€
        status_val = 0 if span['status'] == '200' else 1
        
        # è·å–ID
        operation_id = operation_name_to_id.get(span['operationName'], 0)
        service_id = service_name_to_id.get(span['serviceName'], 0)

        # CSVè®°å½•åŒ…å«çœŸå®æ ‡ç­¾
        csv_record = {
            'traceIdHigh': trace_id_high,
            'traceIdLow': trace_id_low,
            'parentSpanId': parent_span_id,
            'spanId': span_id,
            'startTime': start_time_str,
            'duration': duration_ms,
            'nanosecond': int(nanosecond),
            'DBhash': 0,
            'status': status_val,
            'operationName': operation_id,
            'serviceName': service_id,
            'nodeLatencyLabel': span['nodeLatencyLabel'],
            'graphLatencyLabel': span['graphLatencyLabel'],
            'graphStructureLabel': span['graphStructureLabel']
        }
        
        csv_records.append(csv_record)
    
    return pd.DataFrame(csv_records)

def split_dataset_preserve_labels(df):
    """åˆ†å‰²æ•°æ®é›†ï¼Œä¿ç•™æ ‡ç­¾åˆ†å¸ƒ"""
    print("åˆ†å‰²æ•°æ®é›†ï¼ˆä¿ç•™æ ‡ç­¾åˆ†å¸ƒï¼‰...")
    
    # æŒ‰traceåˆ†ç»„
    trace_groups = df.groupby(['traceIdHigh', 'traceIdLow'])
    
    # åˆ†ç¦»æ­£å¸¸å’Œå¼‚å¸¸traces
    normal_traces = []
    anomaly_traces = []
    
    for trace_id, group in trace_groups:
        has_anomaly = (group['nodeLatencyLabel'].sum() > 0 or 
                      group['graphLatencyLabel'].sum() > 0 or 
                      group['graphStructureLabel'].sum() > 0)
        
        if has_anomaly:
            anomaly_traces.append(trace_id)
        else:
            normal_traces.append(trace_id)
    
    print(f"æ­£å¸¸traces: {len(normal_traces)}, å¼‚å¸¸traces: {len(anomaly_traces)}")
    
    # åˆ†åˆ«åˆ†å‰²æ­£å¸¸å’Œå¼‚å¸¸traces
    random.shuffle(normal_traces)
    random.shuffle(anomaly_traces)
    
    # æ­£å¸¸tracesåˆ†å‰²
    normal_total = len(normal_traces)
    normal_train_size = int(0.7 * normal_total)
    normal_val_size = int(0.15 * normal_total)
    
    normal_train = normal_traces[:normal_train_size]
    normal_val = normal_traces[normal_train_size:normal_train_size + normal_val_size]
    normal_test = normal_traces[normal_train_size + normal_val_size:]
    
    # å¼‚å¸¸tracesåˆ†å‰²
    anomaly_total = len(anomaly_traces)
    anomaly_train_size = int(0.7 * anomaly_total)
    anomaly_val_size = int(0.15 * anomaly_total)
    
    anomaly_train = anomaly_traces[:anomaly_train_size]
    anomaly_val = anomaly_traces[anomaly_train_size:anomaly_train_size + anomaly_val_size]
    anomaly_test = anomaly_traces[anomaly_train_size + anomaly_val_size:]
    
    # åˆå¹¶è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
    train_traces = normal_train + anomaly_train
    val_traces = normal_val + anomaly_val
    test_traces = normal_test + anomaly_test
    
    # åˆ›å»ºæ•°æ®æ¡†
    train_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(train_traces)].reset_index(drop=True)
    val_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(val_traces)].reset_index(drop=True)
    test_df = df[df.set_index(['traceIdHigh', 'traceIdLow']).index.isin(test_traces)].reset_index(drop=True)
    
    return train_df, val_df, test_df

# ä¿æŒå…¶ä»–è¾…åŠ©å‡½æ•°ä¸å˜
def generate_id_mappings(operations, services, statuses, spans):
    """ç”ŸæˆIDæ˜ å°„æ–‡ä»¶"""
    print("ç”ŸæˆIDæ˜ å°„...")
    
    # æ“ä½œæ˜ å°„
    operation_name_to_id = {}
    for i, op in enumerate(sorted(operations), 1):
        operation_name_to_id[str(op)] = i
    
    # æœåŠ¡æ˜ å°„
    service_name_to_id = {}
    for i, svc in enumerate(sorted(services), 1):
        service_name_to_id[str(svc)] = i
    
    # çŠ¶æ€æ˜ å°„
    status_name_to_id = {}
    for i, status in enumerate(sorted(statuses), 1):
        status_name_to_id[str(status)] = i
    
    # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
    operation_durations = defaultdict(list)
    for span in spans:
        duration_ms = span['duration'] / 1000
        operation_name = span['operationName']
        operation_durations[operation_name].append(duration_ms)
    
    return operation_name_to_id, service_name_to_id, status_name_to_id, operation_durations

def generate_realistic_ids(original_id):
    """ç”Ÿæˆç°å®çš„trace ID"""
    hash_obj = hashlib.md5(str(original_id).encode())
    hash_int = int(hash_obj.hexdigest()[:16], 16)
    
    trace_id_high = (hash_int >> 32) & 0x7FFFFFFFFFFFFFFF
    trace_id_low = hash_int & 0x7FFFFFFFFFFFFFFF
    
    return trace_id_high, trace_id_low

def generate_span_id():
    """ç”Ÿæˆspan ID"""
    return random.randint(-2**31, 2**31-1)

def save_yaml_files(operation_name_to_id, service_name_to_id, status_name_to_id, operation_durations, output_dir):
    """ä¿å­˜YAMLæ˜ å°„æ–‡ä»¶"""
    id_manager_dir = Path(output_dir) / "id_manager"
    id_manager_dir.mkdir(parents=True, exist_ok=True)
    
    # operation_id.yml
    operation_file = id_manager_dir / "operation_id.yml"
    with open(operation_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        composite_key = f"{service_id}/{operation_id}"
                        f.write(f"{composite_key}: {global_id}\n")
                        global_id += 1
    
    # service_id.yml
    service_file = id_manager_dir / "service_id.yml"
    with open(service_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        for svc_id in sorted(service_name_to_id.values()):
            if svc_id != 0:
                f.write(f"'{svc_id}': {svc_id}\n")
    
    # status_id.yml
    status_file = id_manager_dir / "status_id.yml"
    with open(status_file, 'w', encoding='utf-8') as f:
        f.write("? ''\n: 0\n")
        f.write("'0': 1\n")
        f.write("'1': 2\n")
        f.write("'200': 3\n")
    
    # latency_range.yml
    latency_file = id_manager_dir / "latency_range.yml"
    with open(latency_file, 'w', encoding='utf-8') as f:
        global_id = 1
        service_ids = sorted(service_name_to_id.values())
        operation_ids = sorted(operation_name_to_id.values())
        
        for service_id in service_ids:
            if service_id > 0:
                for operation_id in operation_ids:
                    if operation_id > 0:
                        original_op_name = None
                        for op_name, mapped_id in operation_name_to_id.items():
                            if mapped_id == operation_id:
                                original_op_name = op_name
                                break
                        
                        if original_op_name and original_op_name in operation_durations and operation_durations[original_op_name]:
                            durations = operation_durations[original_op_name]
                            mean_val = float(np.mean(durations))
                            std_val = float(np.std(durations)) if len(durations) > 1 else 1.0
                            p99_val = float(np.percentile(durations, 99)) if len(durations) > 1 else mean_val * 3
                        else:
                            mean_val, std_val, p99_val = 1.0, 1.0, 3.0
                        
                        f.write(f"{global_id}:\n")
                        f.write(f"  mean: {mean_val}\n")
                        f.write(f"  std: {std_val}\n")
                        f.write(f"  p99: {p99_val}\n")
                        global_id += 1
    
    print(f"âœ… YAMLæ–‡ä»¶å·²ç”Ÿæˆåˆ°: {id_manager_dir}")

if __name__ == "__main__":
    main()