#!/usr/bin/env python3
"""
åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨ - é€‚é…TraceVAEçŠ¶æ€å­—å…¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import time
import logging
from typing import List, Dict, Any, Tuple
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

from ..utils.preprocessor import TracePreprocessor
from ..config import config

logger = logging.getLogger(__name__)

class TraceVAEInferenceEngine:
    """TraceVAEæ¨ç†å¼•æ“ - åŸºäºé¢„è®­ç»ƒçŠ¶æ€å­—å…¸çš„ç®€åŒ–æ¨ç†"""
    
    def __init__(self, state_dict, device):
        self.device = device
        self.state_dict = state_dict
        
        # æå–å…³é”®ç»„ä»¶
        self.operation_embedding = self._extract_operation_embedding()
        self.struct_features = self._extract_struct_features()
        self.latency_features = self._extract_latency_features()
        
        # åˆ†ææ¨¡å‹ç»“æ„
        self._analyze_model_structure()
        
        logger.info("âœ… TraceVAEæ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
    
    def _analyze_model_structure(self):
        """åˆ†ææ¨¡å‹ç»“æ„"""
        modules = {}
        for key in self.state_dict.keys():
            module_name = key.split('.')[0]
            modules[module_name] = modules.get(module_name, 0) + 1
        
        logger.info("ğŸ§© æ£€æµ‹åˆ°çš„æ¨¡å‹ç»„ä»¶:")
        for module, count in sorted(modules.items()):
            logger.info(f"  - {module}: {count} ä¸ªå‚æ•°")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒVAEç»“æ„
        self.has_struct_vae = any(key.startswith('struct_vae') for key in self.state_dict.keys())
        self.has_latency_vae = any(key.startswith('latency_vae') for key in self.state_dict.keys())
        
        logger.info(f"ğŸ” æ¨¡å‹ç»“æ„åˆ†æ:")
        logger.info(f"  - ç»“æ„VAE: {'âœ“' if self.has_struct_vae else 'âœ—'}")
        logger.info(f"  - å»¶è¿ŸVAE: {'âœ“' if self.has_latency_vae else 'âœ—'}")
    
    def _extract_operation_embedding(self):
        """æå–æ“ä½œåµŒå…¥æƒé‡"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®å
            possible_keys = [
                'operation_embedding.node_embedding.weight',
                'struct_vae.operation_embedding.node_embedding.weight',
                'latency_vae.operation_embedding.node_embedding.weight'
            ]
            
            for key in possible_keys:
                if key in self.state_dict:
                    embedding = self.state_dict[key].to(self.device)
                    logger.info(f"âœ… æå–æ“ä½œåµŒå…¥: {embedding.shape} from {key}")
                    return embedding
            
            logger.warning("âš ï¸  æœªæ‰¾åˆ°æ“ä½œåµŒå…¥æƒé‡")
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸  æå–æ“ä½œåµŒå…¥å¤±è´¥: {e}")
            return None
    
    def _extract_struct_features(self):
        """æå–ç»“æ„ç›¸å…³ç‰¹å¾æƒé‡"""
        struct_weights = {}
        try:
            for key, value in self.state_dict.items():
                if 'struct_vae' in key and ('mean' in key or 'logstd' in key or 'logvar' in key):
                    struct_weights[key] = value.to(self.device)
            
            logger.info(f"âœ… æå–äº† {len(struct_weights)} ä¸ªç»“æ„VAEå‚æ•°")
            return struct_weights
            
        except Exception as e:
            logger.warning(f"âš ï¸  æå–ç»“æ„ç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    def _extract_latency_features(self):
        """æå–å»¶è¿Ÿç›¸å…³ç‰¹å¾æƒé‡"""
        latency_weights = {}
        try:
            for key, value in self.state_dict.items():
                if 'latency_vae' in key and ('mean' in key or 'logstd' in key or 'logvar' in key):
                    latency_weights[key] = value.to(self.device)
            
            logger.info(f"âœ… æå–äº† {len(latency_weights)} ä¸ªå»¶è¿ŸVAEå‚æ•°")
            return latency_weights
            
        except Exception as e:
            logger.warning(f"âš ï¸  æå–å»¶è¿Ÿç‰¹å¾å¤±è´¥: {e}")
            return {}
    
    def compute_operation_embeddings(self, operation_ids):
        """è®¡ç®—æ“ä½œåµŒå…¥"""
        if self.operation_embedding is None:
            # è¿”å›éšæœºåµŒå…¥ä½œä¸ºfallback
            return torch.randn(len(operation_ids), 40).to(self.device) * 0.1
        
        # ç¡®ä¿operation_idsåœ¨æœ‰æ•ˆèŒƒå›´å†…
        max_ops = self.operation_embedding.shape[0]
        operation_ids = torch.clamp(operation_ids, 0, max_ops - 1)
        
        return self.operation_embedding[operation_ids]
    
    def compute_anomaly_scores(self, trace_features, operation_ids, service_ids):
        """è®¡ç®—å¼‚å¸¸åˆ†æ•°"""
        try:
            batch_size = trace_features.shape[0]
            
            # 1. è®¡ç®—æ“ä½œåµŒå…¥
            op_embeddings = self.compute_operation_embeddings(operation_ids)
            
            # 2. åŸºäºåµŒå…¥çš„ç»“æ„å¼‚å¸¸æ£€æµ‹
            struct_score = self._compute_structure_anomaly_score(
                trace_features, op_embeddings, operation_ids, service_ids
            )
            
            # 3. åŸºäºå»¶è¿Ÿçš„æ—¶é—´å¼‚å¸¸æ£€æµ‹
            latency_score = self._compute_latency_anomaly_score(trace_features)
            
            # 4. ç»¼åˆå¼‚å¸¸åˆ†æ•°
            if self.has_struct_vae and self.has_latency_vae:
                # åŒVAEæ¶æ„ï¼šç»“æ„å’Œå»¶è¿Ÿåˆ†å¼€æ£€æµ‹
                total_score = 0.6 * struct_score + 0.4 * latency_score
            elif self.has_struct_vae:
                # åªæœ‰ç»“æ„VAE
                total_score = struct_score
                latency_score = struct_score * 0.5  # ä¼°ç®—
            else:
                # ç»Ÿä¸€æ¶æ„æˆ–æœªçŸ¥
                total_score = (struct_score + latency_score) / 2
            
            return {
                'reconstruction_error': float(latency_score),
                'kl_divergence': float(struct_score),
                'total_loss': float(total_score)
            }
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›ä¸­ç­‰å¼‚å¸¸åˆ†æ•°ä½œä¸ºfallback
            return {
                'reconstruction_error': 0.3,
                'kl_divergence': 0.2,
                'total_loss': 0.25
            }
    
    def _compute_structure_anomaly_score(self, features, op_embeddings, operation_ids, service_ids):
        """è®¡ç®—ç»“æ„å¼‚å¸¸åˆ†æ•°"""
        try:
            # 1. æ£€æŸ¥ä¾èµ–å…³ç³»çš„åˆç†æ€§
            parent_features = features[:, 2] if features.shape[1] > 2 else torch.zeros(features.shape[0])
            has_parent_ratio = (parent_features != 0).float().mean()
            
            # 2. æ£€æŸ¥æ“ä½œç±»å‹çš„å¤šæ ·æ€§
            unique_ops = len(torch.unique(operation_ids))
            op_diversity = unique_ops / len(operation_ids) if len(operation_ids) > 0 else 0
            
            # 3. æ£€æŸ¥æœåŠ¡çš„å¤šæ ·æ€§
            unique_services = len(torch.unique(service_ids))
            service_diversity = unique_services / len(service_ids) if len(service_ids) > 0 else 0
            
            # 4. åŸºäºæ“ä½œåµŒå…¥çš„å¼‚å¸¸æ£€æµ‹
            if op_embeddings.shape[0] > 1:
                # è®¡ç®—åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§
                embedding_norm = F.normalize(op_embeddings, dim=1)
                similarity_matrix = torch.mm(embedding_norm, embedding_norm.t())
                
                # æ’é™¤å¯¹è§’çº¿å…ƒç´ 
                mask = ~torch.eye(similarity_matrix.shape[0], dtype=torch.bool)
                similarities = similarity_matrix[mask]
                
                # å¦‚æœæ‰€æœ‰æ“ä½œéƒ½éå¸¸ç›¸ä¼¼ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸
                high_similarity_ratio = (similarities > 0.9).float().mean()
                embedding_anomaly = high_similarity_ratio
            else:
                embedding_anomaly = 0.0
            
            # ç»¼åˆç»“æ„å¼‚å¸¸åˆ†æ•°
            structure_anomalies = []
            
            # ä¾èµ–å…³ç³»å¼‚å¸¸ï¼šå¤ªå°‘æˆ–å¤ªå¤šçš„çˆ¶å­å…³ç³»
            if has_parent_ratio < 0.1 or has_parent_ratio > 0.95:
                structure_anomalies.append(0.8)
            else:
                structure_anomalies.append(0.1)
            
            # å¤šæ ·æ€§å¼‚å¸¸ï¼šæ“ä½œæˆ–æœåŠ¡ç±»å‹è¿‡äºå•ä¸€
            if op_diversity < 0.3 or service_diversity < 0.3:
                structure_anomalies.append(0.6)
            else:
                structure_anomalies.append(0.1)
            
            # åµŒå…¥å¼‚å¸¸
            structure_anomalies.append(float(embedding_anomaly) * 0.7)
            
            # è¿”å›æœ€å¤§å¼‚å¸¸åˆ†æ•°
            return max(structure_anomalies)
            
        except Exception as e:
            logger.warning(f"ç»“æ„å¼‚å¸¸åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.2
    
    def _compute_latency_anomaly_score(self, features):
        """è®¡ç®—å»¶è¿Ÿå¼‚å¸¸åˆ†æ•°"""
        try:
            # æå–durationä¿¡æ¯
            durations = features[:, 0] if features.shape[1] > 0 else torch.tensor([1000.0])
            
            if len(durations) == 0:
                return 0.2
            
            # 1. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
            if len(durations) > 1:
                mean_duration = torch.mean(durations)
                std_duration = torch.std(durations)
                
                # Z-scoreå¼‚å¸¸æ£€æµ‹
                z_scores = torch.abs(durations - mean_duration) / (std_duration + 1e-6)
                max_z_score = torch.max(z_scores)
                
                # è½¬æ¢ä¸ºå¼‚å¸¸åˆ†æ•°
                z_score_anomaly = torch.clamp(max_z_score / 5.0, 0.0, 1.0)
            else:
                z_score_anomaly = 0.1
            
            # 2. ç»å¯¹å€¼å¼‚å¸¸æ£€æµ‹
            # å¦‚æœæœ‰ç‰¹åˆ«å¤§çš„å»¶è¿Ÿå€¼ï¼ˆæ¯”å¦‚è¶…è¿‡30ç§’ï¼‰
            large_latency_ratio = (durations > 30000).float().mean()  # 30ç§’
            
            # 3. å»¶è¿Ÿæ¨¡å¼å¼‚å¸¸
            # æ£€æŸ¥æ˜¯å¦æœ‰çªç„¶çš„å»¶è¿Ÿè·³è·ƒ
            if len(durations) > 2:
                duration_diffs = torch.abs(durations[1:] - durations[:-1])
                max_jump = torch.max(duration_diffs) / (torch.mean(durations) + 1e-6)
                jump_anomaly = torch.clamp(max_jump / 10.0, 0.0, 1.0)
            else:
                jump_anomaly = 0.0
            
            # ç»¼åˆå»¶è¿Ÿå¼‚å¸¸åˆ†æ•°
            latency_anomalies = [
                float(z_score_anomaly),
                float(large_latency_ratio) * 0.9,
                float(jump_anomaly),
            ]
            
            return max(latency_anomalies)
            
        except Exception as e:
            logger.warning(f"å»¶è¿Ÿå¼‚å¸¸åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.2

class TraceAnomalyDetector:
    """åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, config_dir: str, device: str = "cpu"):
        self.device = torch.device(device)
        self.model_path = Path(model_path)
        self.config_dir = Path(config_dir)
        self.preprocessor = TracePreprocessor(config_dir)
        self.inference_engine = None
        self.executor = ThreadPoolExecutor(max_workers=config.MAX_WORKERS)
        
        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        self.anomaly_threshold = config.ANOMALY_THRESHOLD
        
        self.load_model()
    
    def load_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çŠ¶æ€å­—å…¸"""
        try:
            logger.info(f"ğŸ”„ åŠ è½½TraceVAEçŠ¶æ€å­—å…¸: {self.model_path}")
            
            if not self.model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ è½½çŠ¶æ€å­—å…¸
            state_dict = torch.load(self.model_path, map_location=self.device)
            
            if not isinstance(state_dict, dict):
                raise ValueError(f"æœŸæœ›çŠ¶æ€å­—å…¸ï¼Œä½†å¾—åˆ°: {type(state_dict)}")
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(state_dict)} ä¸ªæ¨¡å‹å‚æ•°")
            
            # åˆ›å»ºæ¨ç†å¼•æ“
            self.inference_engine = TraceVAEInferenceEngine(state_dict, self.device)
            
            logger.info("âœ… TraceVAEæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def detect_anomalies(self, traces: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸"""
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†æ•°æ®
            df = self.preprocessor.preprocess_traces(traces)
            
            if df.empty:
                return []
            
            # æŒ‰traceåˆ†ç»„è¿›è¡Œæ£€æµ‹
            results = []
            trace_groups = df.groupby('original_trace_id')
            
            for original_trace_id, group in trace_groups:
                trace_start_time = time.time()
                
                # å‡†å¤‡æ¨¡å‹è¾“å…¥
                model_input, operation_ids, service_ids = self._prepare_model_input(group)
                
                # æ¨¡å‹æ¨ç†
                anomaly_scores = self._run_inference(model_input, operation_ids, service_ids)
                
                # è§£æç»“æœ
                result = self._parse_detection_result(
                    original_trace_id, 
                    anomaly_scores, 
                    trace_start_time
                )
                
                results.append(result)
            
            total_time = (time.time() - start_time) * 1000
            logger.info(f"âœ… æ£€æµ‹å®Œæˆ: {len(results)} traces, {total_time:.2f}ms")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            raise
    
    def _prepare_model_input(self, trace_df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        features = []
        operation_ids = []
        service_ids = []
        
        for _, row in trace_df.iterrows():
            # æ„å»ºç‰¹å¾å‘é‡
            feature_vector = [
                float(row.get('duration', 1000)) / 1000.0,     # æ ‡å‡†åŒ–æŒç»­æ—¶é—´
                float(row.get('status', 0)),                   # çŠ¶æ€ç 
                float(row.get('parentSpanId', 0) != 0),        # æ˜¯å¦æœ‰çˆ¶èŠ‚ç‚¹
                float(row.get('nanosecond', 0)) / 1000000.0,   # æ ‡å‡†åŒ–çº³ç§’
            ]
            features.append(feature_vector)
            
            # æ“ä½œå’ŒæœåŠ¡ID
            operation_ids.append(int(row.get('operationName', 0)))
            service_ids.append(int(row.get('serviceName', 0)))
        
        # è½¬æ¢ä¸ºtensor
        if not features:
            return (torch.zeros(1, 4).to(self.device), 
                   torch.zeros(1, dtype=torch.long).to(self.device),
                   torch.zeros(1, dtype=torch.long).to(self.device))
        
        features_tensor = torch.tensor(features, dtype=torch.float32).to(self.device)
        operation_ids_tensor = torch.tensor(operation_ids, dtype=torch.long).to(self.device)
        service_ids_tensor = torch.tensor(service_ids, dtype=torch.long).to(self.device)
        
        return features_tensor, operation_ids_tensor, service_ids_tensor
    
    def _run_inference(self, model_input: torch.Tensor, operation_ids: torch.Tensor, service_ids: torch.Tensor) -> Dict[str, float]:
        """è¿è¡Œæ¨¡å‹æ¨ç†"""
        try:
            with torch.no_grad():
                scores = self.inference_engine.compute_anomaly_scores(
                    model_input, operation_ids, service_ids
                )
                return scores
                
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            # è¿”å›ä¸­ç­‰å¼‚å¸¸åˆ†æ•°ä½œä¸ºfallback
            return {
                'reconstruction_error': 0.3,
                'kl_divergence': 0.2,
                'total_loss': 0.25
            }
    
    def _parse_detection_result(
        self, 
        trace_id: str, 
        scores: Dict[str, float], 
        start_time: float
    ) -> Dict[str, Any]:
        """è§£ææ£€æµ‹ç»“æœ"""
        processing_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # å¼‚å¸¸åˆ¤æ–­
        total_score = scores.get('total_loss', 0)
        is_anomaly = total_score > self.anomaly_threshold
        
        # ç¡®å®šå¼‚å¸¸ç±»å‹å’Œç½®ä¿¡åº¦
        anomaly_type = "normal"
        confidence = max(0.0, 1.0 - min(total_score / self.anomaly_threshold, 1.0))
        
        if is_anomaly:
            # æ ¹æ®åˆ†æ•°ç‰¹å¾åˆ¤æ–­å¼‚å¸¸ç±»å‹
            recon_error = scores.get('reconstruction_error', 0)
            kl_div = scores.get('kl_divergence', 0)
            
            if kl_div > recon_error and kl_div > 0.4:
                anomaly_type = "structure"  # ç»“æ„å¼‚å¸¸
            elif recon_error > 0.4:
                anomaly_type = "time"  # æ—¶é—´å¼‚å¸¸
            else:
                anomaly_type = "mixed"  # æ··åˆå¼‚å¸¸
            
            confidence = min(total_score / self.anomaly_threshold, 1.0)
        
        return {
            'traceID': trace_id,
            'is_anomaly': is_anomaly,
            'anomaly_type': anomaly_type,
            'confidence': round(confidence, 4),
            'details': {
                'nodeLatencyScore': round(scores.get('reconstruction_error', 0), 4),
                'graphLatencyScore': round(scores.get('kl_divergence', 0), 4),
                'graphStructureScore': round(scores.get('total_loss', 0), 4)
            },
            'processing_time_ms': round(processing_time, 2)
        }