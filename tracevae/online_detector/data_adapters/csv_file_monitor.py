#!/usr/bin/env python3
"""
CSVæ–‡ä»¶ç›‘æ§å™¨ - ç›‘æ§æŒ‡å®šè·¯å¾„çš„CSVæ–‡ä»¶ï¼Œå®æ—¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹

ä½¿ç”¨æ–¹æ³•ï¼š
# ä¸€æ¬¡æ€§æ£€æµ‹CSVæ–‡ä»¶
python csv_file_monitor.py your_data.csv --mode once

# æŒç»­ç›‘æ§CSVæ–‡ä»¶
python csv_file_monitor.py your_data.csv --mode monitor
"""

import asyncio
import aiohttp
import pandas as pd
import json
import time
import os
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import logging
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CSVFileHandler(FileSystemEventHandler):
    """CSVæ–‡ä»¶å˜åŒ–å¤„ç†å™¨"""
    
    def __init__(self, file_monitor):
        self.file_monitor = file_monitor
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹æ—¶è§¦å‘"""
        if not event.is_directory and event.src_path.endswith('.csv'):
            logger.info(f"ğŸ“ æ£€æµ‹åˆ°CSVæ–‡ä»¶å˜åŒ–: {event.src_path}")
            asyncio.create_task(self.file_monitor.process_csv_file(event.src_path))

class CSVFileMonitor:
    """CSVæ–‡ä»¶ç›‘æ§å™¨"""
    
    def __init__(self, service_url: str = "http://localhost:8000"):
        self.service_url = service_url
        self.processed_lines = {}  # è®°å½•æ¯ä¸ªæ–‡ä»¶å·²å¤„ç†çš„è¡Œæ•°
        
        # æ“ä½œå’ŒæœåŠ¡æ˜ å°„
        self.operation_mappings = {
            3: "GET /api/gateway",
            6: "POST /api/auth", 
            17: "SELECT database_query"
        }
        
        self.service_mappings = {
            2: "gateway-service",
            4: "database-service"
        }
    
    async def monitor_csv_file(self, csv_file_path: str, check_interval: int = 5):
        """ç›‘æ§å•ä¸ªCSVæ–‡ä»¶ï¼Œå®šæœŸæ£€æŸ¥æ–°å¢çš„æ•°æ®"""
        csv_path = Path(csv_file_path)
        
        if not csv_path.exists():
            logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return
        
        logger.info(f"ğŸ” å¼€å§‹ç›‘æ§CSVæ–‡ä»¶: {csv_path}")
        
        # è®°å½•åˆå§‹æ–‡ä»¶å¤§å°
        last_size = csv_path.stat().st_size
        last_modified = csv_path.stat().st_mtime
        
        while True:
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
                current_size = csv_path.stat().st_size
                current_modified = csv_path.stat().st_mtime
                
                if current_size > last_size or current_modified > last_modified:
                    logger.info(f"ğŸ“Š æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–ï¼Œå¼€å§‹å¤„ç†æ–°æ•°æ®...")
                    
                    # å¤„ç†æ–°å¢çš„æ•°æ® - è¿”å›æ•´ä¸ªCSVçš„çŠ¶æ€
                    csv_status = await self.process_new_data(csv_path)
                    
                    # è¾“å‡ºCSVæ•´ä½“çŠ¶æ€
                    if csv_status:
                        logger.info(f"ğŸ“ CSVæ–‡ä»¶çŠ¶æ€: {csv_status}")
                    
                    last_size = current_size
                    last_modified = current_modified
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(check_interval)
                
            except Exception as e:
                logger.error(f"âŒ ç›‘æ§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                await asyncio.sleep(check_interval)
    
    async def process_new_data(self, csv_path: Path):
        """å¤„ç†CSVæ–‡ä»¶ä¸­çš„æ–°æ•°æ®ï¼Œè¿”å›æ•´ä¸ªCSVçš„çŠ¶æ€"""
        try:
            # è¯»å–å®Œæ•´æ–‡ä»¶
            df = pd.read_csv(csv_path)
            
            # è·å–æ–‡ä»¶çš„å¤„ç†è®°å½•
            file_key = str(csv_path)
            last_processed = self.processed_lines.get(file_key, 0)
            
            # åªå¤„ç†æ–°å¢çš„è¡Œ
            if len(df) > last_processed:
                new_data = df.iloc[last_processed:]
                logger.info(f"ğŸ“ˆ å‘ç° {len(new_data)} è¡Œæ–°æ•°æ®")
                
                # è½¬æ¢ä¸ºtracesæ ¼å¼
                traces = self.convert_csv_to_traces(new_data)
                
                if traces:
                    # å‘é€åˆ°æ£€æµ‹æœåŠ¡ï¼Œè·å–æ¯ä¸ªtraceçš„æ£€æµ‹ç»“æœ
                    trace_results = await self.send_for_detection(traces)
                    
                    # åˆ¤æ–­æ•´ä¸ªCSVæ–‡ä»¶çš„çŠ¶æ€
                    csv_status = self.determine_csv_status(trace_results, csv_path.name)
                    
                    # æ›´æ–°å¤„ç†è®°å½•
                    self.processed_lines[file_key] = len(df)
                    
                    return csv_status
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–°æ•°æ®å¤±è´¥: {e}")
            return {"status": "ERROR", "error": str(e)}
    
    async def process_entire_csv_file(self, csv_file_path: str, batch_size: int = 20):
        """ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªCSVæ–‡ä»¶ï¼Œè¿”å›æ•´ä¸ªCSVçš„çŠ¶æ€"""
        csv_path = Path(csv_file_path)
        
        if not csv_path.exists():
            logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            return {"status": "ERROR", "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        logger.info(f"ğŸ“‚ å¼€å§‹å¤„ç†CSVæ–‡ä»¶: {csv_path}")
        
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_path)
            logger.info(f"ğŸ“Š æ–‡ä»¶åŒ…å« {len(df)} è¡Œæ•°æ®")
            
            # è½¬æ¢ä¸ºtraces
            traces = self.convert_csv_to_traces(df)
            logger.info(f"ğŸ”„ è½¬æ¢å¾—åˆ° {len(traces)} ä¸ªtraces")
            
            if not traces:
                logger.warning("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„traceæ•°æ®")
                return {"status": "NORMAL", "reason": "æ— æœ‰æ•ˆæ•°æ®"}
            
            # åˆ†æ‰¹å¤„ç†
            all_trace_results = []
            for i in range(0, len(traces), batch_size):
                batch = traces[i:i + batch_size]
                logger.info(f"ğŸ” å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} traces")
                
                batch_results = await self.send_for_detection(batch)
                if batch_results:
                    all_trace_results.extend(batch_results)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.5)
            
            # åˆ¤æ–­æ•´ä¸ªCSVæ–‡ä»¶çš„çŠ¶æ€
            csv_status = self.determine_csv_status(all_trace_results, csv_path.name)
            
            # è¾“å‡ºç»“æœ
            self.print_csv_result(csv_status)
            
            return csv_status
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†CSVæ–‡ä»¶å¤±è´¥: {e}")
            return {"status": "ERROR", "error": str(e)}
    
    def determine_csv_status(self, trace_results: List[Dict], csv_filename: str) -> Dict[str, Any]:
        """
        æ ¹æ®traceæ£€æµ‹ç»“æœåˆ¤æ–­æ•´ä¸ªCSVæ–‡ä»¶çš„çŠ¶æ€
        åªè¦æœ‰ä¸€ä¸ªtraceå¼‚å¸¸ï¼Œæ•´ä¸ªCSVå°±æ˜¯å¼‚å¸¸
        """
        if not trace_results:
            return {
                "csv_file": csv_filename,
                "status": "NORMAL", 
                "reason": "æ— æ£€æµ‹ç»“æœ",
                "total_traces": 0,
                "anomaly_traces": 0,
                "normal_traces": 0,
                "anomaly_percentage": 0.0
            }
        
        total_traces = len(trace_results)
        anomaly_traces = sum(1 for r in trace_results if r.get('is_anomaly', False))
        normal_traces = total_traces - anomaly_traces
        anomaly_percentage = (anomaly_traces / total_traces) * 100
        
        # å…³é”®é€»è¾‘ï¼šåªè¦æœ‰ä¸€ä¸ªtraceå¼‚å¸¸ï¼Œæ•´ä¸ªCSVå°±æ˜¯å¼‚å¸¸
        csv_is_anomaly = anomaly_traces > 0
        
        # ç»Ÿè®¡å¼‚å¸¸ç±»å‹
        anomaly_types = {}
        anomaly_details = []
        
        for result in trace_results:
            if result.get('is_anomaly', False):
                anomaly_type = result.get('anomaly_type', 'unknown')
                anomaly_types[anomaly_type] = anomaly_types.get(anomaly_type, 0) + 1
                
                anomaly_details.append({
                    "traceID": result.get('traceID'),
                    "anomaly_type": anomaly_type,
                    "confidence": result.get('confidence', 0)
                })
        
        return {
            "csv_file": csv_filename,
            "status": "ANOMALY" if csv_is_anomaly else "NORMAL",
            "total_traces": total_traces,
            "anomaly_traces": anomaly_traces,
            "normal_traces": normal_traces,
            "anomaly_percentage": anomaly_percentage,
            "anomaly_types": anomaly_types,
            "anomaly_details": anomaly_details[:10],  # åªä¿ç•™å‰10ä¸ªå¼‚å¸¸è¯¦æƒ…
            "detection_timestamp": datetime.utcnow().isoformat()
        }
    
    def print_csv_result(self, csv_status: Dict[str, Any]):
        """æ‰“å°CSVæ£€æµ‹ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š CSVæ–‡ä»¶å¼‚å¸¸æ£€æµ‹ç»“æœ")
        print("="*60)
        
        status_icon = "ğŸš¨" if csv_status["status"] == "ANOMALY" else "âœ…"
        print(f"{status_icon} CSVæ–‡ä»¶: {csv_status['csv_file']}")
        print(f"ğŸ“‹ æ•´ä½“çŠ¶æ€: {csv_status['status']}")
        print(f"ğŸ“ˆ æ€»traces: {csv_status['total_traces']}")
        print(f"âœ… æ­£å¸¸traces: {csv_status['normal_traces']}")
        print(f"ğŸš¨ å¼‚å¸¸traces: {csv_status['anomaly_traces']}")
        print(f"ğŸ“Š å¼‚å¸¸æ¯”ä¾‹: {csv_status['anomaly_percentage']:.1f}%")
        
        if csv_status["status"] == "ANOMALY":
            print(f"\nğŸ·ï¸  å¼‚å¸¸ç±»å‹åˆ†å¸ƒ:")
            for anomaly_type, count in csv_status.get('anomaly_types', {}).items():
                print(f"  - {anomaly_type}: {count}")
            
            print(f"\nğŸ” å¼‚å¸¸traceç¤ºä¾‹:")
            for detail in csv_status.get('anomaly_details', [])[:5]:
                print(f"  - {detail['traceID']}: {detail['anomaly_type']} "
                      f"(ç½®ä¿¡åº¦: {detail['confidence']:.3f})")
        
        # ä¿å­˜ç»“æœ
        output_file = f"csv_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(csv_status, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ... å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ ...
    def convert_csv_to_traces(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """å°†CSVæ•°æ®è½¬æ¢ä¸ºtracesæ ¼å¼"""
        traces = []
        
        try:
            # æ•°æ®ç±»å‹è½¬æ¢
            df = self._convert_data_types(df)
            
            # æŒ‰traceåˆ†ç»„
            trace_groups = df.groupby(['traceIdHigh', 'traceIdLow'])
            
            for (trace_high, trace_low), group in trace_groups:
                trace = self._convert_group_to_trace(group, trace_high, trace_low)
                if trace and len(trace['spans']) > 0:
                    traces.append(trace)
            
            return traces
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢CSVæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _convert_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """è½¬æ¢æ•°æ®ç±»å‹"""
        try:
            # è½¬æ¢æ•°å€¼åˆ—
            numeric_columns = ['traceIdHigh', 'traceIdLow', 'parentSpanId', 'spanId', 
                             'duration', 'nanosecond', 'DBhash', 'status', 'operationName', 'serviceName']
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # è½¬æ¢æ—¶é—´åˆ—
            if 'startTime' in df.columns:
                df['startTime'] = pd.to_datetime(df['startTime'], errors='coerce')
            
            return df
        except Exception as e:
            logger.warning(f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {e}")
            return df
    
    def _convert_group_to_trace(self, group: pd.DataFrame, trace_high: int, trace_low: int) -> Dict[str, Any]:
        """å°†groupedæ•°æ®è½¬æ¢ä¸ºtraceæ ¼å¼"""
        try:
            trace_id = f"trace_{trace_high}_{trace_low}"
            spans = []
            processes = {}
            
            # æŒ‰æ—¶é—´æ’åºspans
            group_sorted = group.sort_values(['startTime', 'nanosecond'])
            
            for _, row in group_sorted.iterrows():
                span = self._convert_row_to_span(row)
                if span:
                    spans.append(span)
                    
                    # æ·»åŠ åˆ°processes
                    service_name = span['serviceName']
                    process_key = f"p{len(processes)}"
                    if service_name not in [p.get('serviceName') for p in processes.values()]:
                        processes[process_key] = {"serviceName": service_name}
            
            if not spans:
                return None
            
            return {
                "traceID": trace_id,
                "spans": spans,
                "processes": processes
            }
            
        except Exception as e:
            logger.warning(f"è½¬æ¢traceå¤±è´¥: {e}")
            return None
    
    def _convert_row_to_span(self, row) -> Dict[str, Any]:
        """å°†å•è¡Œæ•°æ®è½¬æ¢ä¸ºspanæ ¼å¼"""
        try:
            # Span IDs
            span_id = str(int(row['spanId']))
            parent_span_id = ""
            if row['parentSpanId'] != 0:
                parent_span_id = str(int(row['parentSpanId']))
            
            # æ“ä½œå’ŒæœåŠ¡åç§°
            operation_id = int(row['operationName'])
            service_id = int(row['serviceName'])
            
            operation_name = self.operation_mappings.get(operation_id, f"operation_{operation_id}")
            service_name = self.service_mappings.get(service_id, f"service_{service_id}")
            
            # æ—¶é—´è½¬æ¢
            start_time = self._convert_start_time(row)
            
            # æŒç»­æ—¶é—´è½¬æ¢ï¼ˆæ¯«ç§’åˆ°å¾®ç§’ï¼‰
            duration_ms = int(row['duration'])
            duration_us = duration_ms * 1000
            
            # çŠ¶æ€è½¬æ¢
            status = int(row['status'])
            status_code = 500 if status == 1 else 200
            
            # æ„å»ºspan
            span = {
                "spanID": span_id,
                "parentSpanID": parent_span_id,
                "operationName": operation_name,
                "serviceName": service_name,
                "startTime": start_time,
                "duration": duration_us,
                "tags": [
                    {"key": "http.status_code", "value": status_code},
                    {"key": "operation.id", "value": operation_id},
                    {"key": "service.id", "value": service_id}
                ]
            }
            
            return span
            
        except Exception as e:
            logger.warning(f"è½¬æ¢spanå¤±è´¥: {e}")
            return None
    
    def _convert_start_time(self, row) -> int:
        """è½¬æ¢å¼€å§‹æ—¶é—´ä¸ºå¾®ç§’æ—¶é—´æˆ³"""
        try:
            start_time = row['startTime']
            nanosecond = int(row['nanosecond'])
            
            if pd.isna(start_time):
                return int(datetime.now().timestamp() * 1000000)
            
            # è½¬æ¢ä¸ºå¾®ç§’æ—¶é—´æˆ³
            timestamp_us = int(start_time.timestamp() * 1000000)
            timestamp_us += nanosecond // 1000
            
            return timestamp_us
            
        except Exception as e:
            return int(datetime.now().timestamp() * 1000000)
    
    async def send_for_detection(self, traces: List[Dict]) -> List[Dict]:
        """å‘é€tracesåˆ°æ£€æµ‹æœåŠ¡"""
        try:
            request_data = {
                "traces": traces,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                
                async with session.post(
                    f"{self.service_url}/detect",
                    json=request_data,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    request_time = (time.time() - start_time) * 1000
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        anomaly_count = result.get('total_anomalies', 0)
                        total_count = result.get('total_processed', 0)
                        
                        logger.info(f"âœ… æ£€æµ‹å®Œæˆ: {total_count} traces, "
                                  f"{anomaly_count} å¼‚å¸¸, {request_time:.0f}ms")
                        
                        return result.get('results', [])
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ æ£€æµ‹è¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                        return []
                        
        except Exception as e:
            logger.error(f"âŒ å‘é€æ£€æµ‹è¯·æ±‚å¤±è´¥: {e}")
            return []

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CSVæ–‡ä»¶å¼‚å¸¸æ£€æµ‹ç›‘æ§å™¨")
    parser.add_argument("csv_file", help="è¦ç›‘æ§çš„CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--mode", choices=["once", "monitor"], default="once",
                       help="è¿è¡Œæ¨¡å¼: once=ä¸€æ¬¡æ€§å¤„ç†, monitor=æŒç»­ç›‘æ§")
    parser.add_argument("--service-url", default="http://localhost:8000",
                       help="æ£€æµ‹æœåŠ¡URL")
    parser.add_argument("--batch-size", type=int, default=20,
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--check-interval", type=int, default=5,
                       help="ç›‘æ§æ¨¡å¼ä¸‹çš„æ£€æŸ¥é—´éš”(ç§’)")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    csv_path = Path(args.csv_file)
    if not csv_path.exists():
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = CSVFileMonitor(args.service_url)
    
    # æ£€æŸ¥æ£€æµ‹æœåŠ¡
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{args.service_url}/health") as response:
                if response.status != 200:
                    print(f"âŒ æ£€æµ‹æœåŠ¡ä¸å¯ç”¨: {args.service_url}")
                    print("è¯·å…ˆå¯åŠ¨æ£€æµ‹æœåŠ¡: python run.py")
                    return
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æ£€æµ‹æœåŠ¡: {e}")
        print("è¯·å…ˆå¯åŠ¨æ£€æµ‹æœåŠ¡: python run.py")
        return
    
    if args.mode == "once":
        print(f"ğŸ”„ ä¸€æ¬¡æ€§å¤„ç†CSVæ–‡ä»¶: {csv_path}")
        csv_result = await monitor.process_entire_csv_file(args.csv_file, args.batch_size)
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: CSVæ–‡ä»¶ '{csv_path.name}' çŠ¶æ€ä¸º {csv_result['status']}")
        
    elif args.mode == "monitor":
        print(f"ğŸ‘ï¸  æŒç»­ç›‘æ§CSVæ–‡ä»¶: {csv_path}")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        await monitor.monitor_csv_file(args.csv_file, args.check_interval)

if __name__ == "__main__":
    # å®‰è£…ä¾èµ–æ£€æŸ¥
    try:
        import watchdog
    except ImportError:
        print("éœ€è¦å®‰è£…watchdog: pip install watchdog")
        exit(1)
    
    asyncio.run(main())