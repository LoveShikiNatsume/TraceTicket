# -*- coding: utf-8 -*-
"""
Train Ticket Trace Collector for Anomaly Detection
é‡‡é›† Train Ticket ç³»ç»Ÿçš„é“¾è·¯è¿½è¸ªæ•°æ®ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹åˆ†æ

Author: LoveShikiNatsume
Date: 2025-06-18
Version: 2.0 - æ”¯æŒè·¨æ—¥æœŸè¿è¡Œ
"""

import requests
import json
import time
import logging
import os
import csv
import hashlib
import re
from datetime import datetime
from typing import List, Dict, Optional
from config import Config

class AnomalyDetectionTraceCollector:
    """å¼‚å¸¸æ£€æµ‹é“¾è·¯è¿½è¸ªæ•°æ®é‡‡é›†å™¨ - æ”¯æŒè·¨æ—¥æœŸè¿è¡Œ"""
    
    def __init__(self):
        self.config = Config()
        self.base_url = f"http://{self.config.JAEGER_HOST}:{self.config.JAEGER_PORT}"
        self.api_url = f"{self.base_url}/jaeger/api"
        
        self.base_output_dir = self.config.ensure_output_dir()
        
        self.logger = self._setup_logging()
        self.session = requests.Session()
        self.session.timeout = self.config.REQUEST_TIMEOUT
        
        # ç¼–ç æ˜ å°„å­—å…¸ï¼ˆå…¨å±€ç»´æŠ¤ï¼Œè·¨æ—¥æœŸä¿æŒä¸€è‡´ï¼‰
        self.operation_encoder = {}
        self.service_encoder = {}
        self.operation_counter = 1
        self.service_counter = 1
        
        self.collected_data = []
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_traces": 0,
            "total_spans": 0,
            "error_spans": 0,
            "spans_with_parent": 0,
            "db_spans": 0
        }
        
        self.logger.info(f"Train Ticket é“¾è·¯è¿½è¸ªé‡‡é›†å™¨å·²åˆå§‹åŒ–")
        self.logger.info(f"Jaeger API: {self.api_url}")
        self.logger.info(f"åŸºç¡€è¾“å‡ºç›®å½•: {self.base_output_dir}")

    def _get_current_date_dirs(self):
        """è·å–å½“å‰æ—¥æœŸçš„ç›®å½•è·¯å¾„"""
        today = datetime.now().strftime("%Y-%m-%d")
        today_dir = os.path.join(self.base_output_dir, today)
        csv_dir = os.path.join(today_dir, "csv")
        # ç§»é™¤json_dirï¼Œä¸å†ç”ŸæˆJSONæ–‡ä»¶å¤¹
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(csv_dir, exist_ok=True)
        
        return today, today_dir, csv_dir

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è¾“å‡º"""
        logger = logging.getLogger('TraceCollector')
        logger.setLevel(logging.INFO)
        
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        return logger

    def test_connection(self) -> bool:
        """æµ‹è¯•ä¸ Jaeger çš„è¿æ¥"""
        try:
            response = self.session.get(f"{self.api_url}/services")
            if response.status_code == 200:
                data = response.json()
                services = data.get("data", [])
                trainticket_services = [s for s in services if "trainticket" in s]
                self.logger.info(f"è¿æ¥æˆåŠŸï¼Œå‘ç° {len(trainticket_services)} ä¸ª Train Ticket æœåŠ¡")
                return len(trainticket_services) > 0
            else:
                self.logger.error(f"è¿æ¥å¤±è´¥: HTTP {response.status_code}")
                return False
        except Exception as e:
            self.logger.error(f"è¿æ¥æµ‹è¯•é”™è¯¯: {e}")
            return False

    def get_available_services(self) -> List[str]:
        """è·å–å¯ç”¨çš„ Train Ticket æœåŠ¡åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.api_url}/services")
            if response.status_code == 200:
                all_services = response.json().get("data", [])
                return [s for s in all_services if "trainticket" in s]
            return []
        except Exception as e:
            self.logger.error(f"è·å–æœåŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def collect_traces(self, service: str = None, lookback: str = "5m", limit: int = 100) -> List[Dict]:
        """ä»æŒ‡å®šæœåŠ¡é‡‡é›†é“¾è·¯è¿½è¸ªæ•°æ®"""
        try:
            params = {"lookback": lookback, "limit": limit}
            if service:
                params["service"] = service
            
            self.stats["total_requests"] += 1
            response = self.session.get(f"{self.api_url}/traces", params=params)
            
            if response.status_code == 200:
                traces = response.json().get("data", [])
                self.stats["successful_requests"] += 1
                self.stats["total_traces"] += len(traces)
                return traces
            else:
                self.stats["failed_requests"] += 1
                return []
        except Exception as e:
            self.stats["failed_requests"] += 1
            return []

    def _encode_operation(self, operation_name: str) -> int:
        """å°†æ“ä½œåç¼–ç ä¸ºæ•°å­—"""
        if not operation_name:
            operation_name = "unknown"
        if operation_name not in self.operation_encoder:
            self.operation_encoder[operation_name] = self.operation_counter
            self.operation_counter += 1
        return self.operation_encoder[operation_name]

    def _encode_service(self, service_name: str) -> int:
        """å°†æœåŠ¡åç¼–ç ä¸ºæ•°å­—"""
        if not service_name:
            service_name = "unknown"
        if service_name not in self.service_encoder:
            self.service_encoder[service_name] = self.service_counter
            self.service_counter += 1
        return self.service_encoder[service_name]

    def _extract_parent_span_id(self, span: Dict) -> str:
        """ä» references å­—æ®µä¸­æå–çˆ¶ span IDï¼ˆIstio ç¯å¢ƒç‰¹æœ‰ï¼‰"""
        references = span.get("references", [])
        
        for ref in references:
            if ref.get("refType") == "CHILD_OF":
                parent_span_id = ref.get("spanID", "")
                if parent_span_id:
                    return parent_span_id
        return ""

    def _extract_service_name(self, span: Dict, tags: Dict[str, str]) -> str:
        """ä» Istio span ä¸­æå–æœåŠ¡åç§°"""
        service_name = tags.get("istio.canonical_service", "")
        if service_name:
            return service_name
        
        operation_name = span.get("operationName", "")
        if operation_name:
            match = re.match(r'(ts-[^.]+)', operation_name)
            if match:
                return match.group(1)
            return operation_name.split('.')[0] if '.' in operation_name else operation_name
        
        return "unknown-service"

    def _split_trace_id(self, trace_id: str) -> tuple:
        """å°† 32 ä½ trace ID åˆ†å‰²ä¸ºé«˜ä½å’Œä½ä½"""
        try:
            if len(trace_id) == 32:
                high = int(trace_id[:16], 16)
                low = int(trace_id[16:], 16)
                return high, low
            elif len(trace_id) == 16:
                return 0, int(trace_id, 16)
            else:
                hash_val = int(hashlib.md5(trace_id.encode()).hexdigest()[:16], 16)
                return 0, hash_val
        except:
            return 0, 0

    def _calculate_db_hash(self, tags: Dict[str, str]) -> int:
        """è®¡ç®—æ•°æ®åº“å“ˆå¸Œå€¼ï¼ŒåŸºäº HTTP è¯·æ±‚ä¿¡æ¯"""
        http_info = []
        for key in ['http.method', 'http.url', 'http.status_code']:
            if key in tags and tags[key]:
                http_info.append(f"{key}:{tags[key]}")
        
        if http_info:
            combined = "|".join(http_info)
            hash_val = int(hashlib.md5(combined.encode()).hexdigest()[:8], 16)
            return hash_val % 1000000
        
        return 0

    def _calculate_node_latency_label(self, duration_ms: float) -> int:
        """è®¡ç®—èŠ‚ç‚¹å»¶è¿Ÿæ ‡ç­¾ï¼š0=å¿«é€Ÿ(<100ms), 1=ä¸­ç­‰(<1s), 2=æ…¢é€Ÿ(>=1s)"""
        if duration_ms < 100:
            return 0
        elif duration_ms < 1000:
            return 1
        else:
            return 2

    def _safe_int_from_hex(self, hex_value, default=0) -> int:
        """å®‰å…¨åœ°å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°"""
        try:
            if hex_value is None or hex_value == "":
                return default
            if isinstance(hex_value, (int, float)):
                return int(hex_value)
            hex_str = str(hex_value)
            if hex_str.startswith('0x'):
                return int(hex_str, 16)
            else:
                return int(hex_str, 16)
        except:
            return default

    def _extract_tags(self, tag_list: List[Dict]) -> Dict[str, str]:
        """æå– span æ ‡ç­¾åˆ°å­—å…¸"""
        tags = {}
        for tag in tag_list:
            try:
                key = str(tag.get("key", ""))
                value = str(tag.get("value", ""))
                tags[key] = value
            except:
                continue
        return tags

    def parse_traces(self, traces: List[Dict]) -> List[Dict]:
        """è§£æé“¾è·¯æ•°æ®ä¸ºå¼‚å¸¸æ£€æµ‹æ‰€éœ€æ ¼å¼"""
        parsed_spans = []
        
        for trace in traces:
            trace_id = trace.get("traceID", "")
            trace_id_high, trace_id_low = self._split_trace_id(trace_id)
            
            for span in trace.get("spans", []):
                try:
                    raw_span_id = span.get("spanID", "")
                    raw_parent_span_id = self._extract_parent_span_id(span)
                    
                    span_id = self._safe_int_from_hex(raw_span_id, 0)
                    parent_span_id = self._safe_int_from_hex(raw_parent_span_id, 0)
                    
                    if parent_span_id != 0:
                        self.stats["spans_with_parent"] += 1
                    
                    start_time_us = span.get("startTime", 0)
                    duration_us = span.get("duration", 0)
                    
                    start_time_formatted = ""
                    nanosecond = 0
                    try:
                        if start_time_us and start_time_us > 0:
                            start_time_us = int(start_time_us)
                            dt = datetime.fromtimestamp(start_time_us / 1000000)
                            start_time_formatted = dt.strftime('%Y-%m-%d %H:%M:%S')
                            nanosecond = (start_time_us % 1000000) * 1000
                        else:
                            start_time_formatted = "1970-01-01 00:00:00"
                            nanosecond = 0
                    except:
                        start_time_formatted = "1970-01-01 00:00:00"
                        nanosecond = 0
                    
                    try:
                        duration_us = int(duration_us) if duration_us else 0
                        duration_ms = duration_us / 1000.0
                    except:
                        duration_ms = 0.0
                    
                    span_tags = self._extract_tags(span.get("tags", []))
                    operation_name = span.get("operationName", "")
                    service_name = self._extract_service_name(span, span_tags)
                    
                    service_encoded = self._encode_service(service_name)
                    operation_encoded = self._encode_operation(operation_name)
                    
                    status = 0
                    http_status = span_tags.get("http.status_code", "200")
                    if http_status and http_status.isdigit():
                        status = int(http_status)
                    
                    has_error = False
                    if span_tags.get("error", "false").lower() == "true" or status >= 400:
                        has_error = True
                        self.stats["error_spans"] += 1
                    
                    node_latency_label = self._calculate_node_latency_label(duration_ms)
                    
                    db_hash = self._calculate_db_hash(span_tags)
                    if db_hash > 0:
                        self.stats["db_spans"] += 1
                    
                    span_data = {
                        "traceIdHigh": trace_id_high,
                        "traceIdLow": trace_id_low,
                        "parentSpanId": parent_span_id,
                        "spanId": span_id,
                        "startTime": start_time_formatted,
                        "duration": int(duration_ms),
                        "nanosecond": nanosecond,
                        "DBhash": db_hash,
                        "status": status,
                        "operationName": operation_encoded,
                        "serviceName": service_encoded,
                        "nodeLatencyLabel": node_latency_label,
                        
                        "_original_trace_id": trace_id,
                        "_original_span_id": raw_span_id,
                        "_original_parent_span_id": raw_parent_span_id,
                        "_original_service_name": service_name,
                        "_original_operation_name": operation_name,
                        "_has_error": has_error,
                        "_collection_timestamp": datetime.now().isoformat()
                    }
                    
                    parsed_spans.append(span_data)
                    
                except Exception as e:
                    continue
        
        self.stats["total_spans"] += len(parsed_spans)
        return parsed_spans

    def save_data(self, data: List[Dict], timestamp: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ - æ”¯æŒè·¨æ—¥æœŸ"""
        if not data:
            return
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡ä¿å­˜æ—¶é‡æ–°è·å–å½“å‰æ—¥æœŸç›®å½•
        today, today_dir, csv_dir = self._get_current_date_dirs()
        
        time_part = timestamp.split("T")[1]
        hour_minute = time_part.split(":")[0] + "_" + time_part.split(":")[1]
        filename = hour_minute
        
        # åªä¿å­˜ CSVï¼Œç§»é™¤JSONä¿å­˜
        csv_file = os.path.join(csv_dir, f"{filename}.csv")
        self._save_csv(data, csv_file)
        
        # ä¿å­˜æ˜ å°„è¡¨ï¼ˆæ¯æ—¥æ›´æ–°ï¼‰
        mapping_file = os.path.join(today_dir, f"mapping_{today.replace('-', '')}.json")
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump({
                "operation_mapping": self.operation_encoder,
                "service_mapping": self.service_encoder,
                "reverse_operation_mapping": {v: k for k, v in self.operation_encoder.items()},
                "reverse_service_mapping": {v: k for k, v in self.service_encoder.items()},
                "last_updated": timestamp
            }, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å·²ä¿å­˜ {len(data)} æ¡ span åˆ° {today}/{filename}")

    def _save_csv(self, data: List[Dict], filepath: str):
        """ä¿å­˜ä¸ºå¼‚å¸¸æ£€æµ‹ä¸“ç”¨çš„ CSV æ ¼å¼"""
        if not data:
            return
        
        fieldnames = [
            "traceIdHigh", "traceIdLow", "parentSpanId", "spanId", 
            "startTime", "duration", "nanosecond", "DBhash", "status",
            "operationName", "serviceName", "nodeLatencyLabel"
        ]
        
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for span in data:
                row = {field: span.get(field, 0) for field in fieldnames}
                writer.writerow(row)

    def start_collection(self, duration_minutes: int = 60, interval_seconds: int = None) -> bool:
        """å¼€å§‹é“¾è·¯è¿½è¸ªæ•°æ®é‡‡é›† - æ”¯æŒè·¨æ—¥æœŸè¿è¡Œ"""
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤é—´éš”
        if interval_seconds is None:
            interval_seconds = self.config.DEFAULT_COLLECTION_INTERVAL
            
        self.logger.info("å¼€å§‹ Train Ticket é“¾è·¯è¿½è¸ªæ•°æ®é‡‡é›†")
        
        if not self.test_connection():
            self.logger.error("æ— æ³•è¿æ¥åˆ° Jaeger")
            return False
        
        services = self.get_available_services()
        if not services:
            self.logger.error("æœªå‘ç° Train Ticket æœåŠ¡")
            return False
        
        if duration_minutes <= 0:
            self.logger.info("æŒç»­è¿è¡Œæ¨¡å¼ï¼ˆduration <= 0ï¼‰ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
            end_time = float('inf')
        else:
            self.logger.info(f"å°†ä» {len(services)} ä¸ªæœåŠ¡é‡‡é›†æ•°æ®ï¼ŒæŒç»­ {duration_minutes} åˆ†é’Ÿ")
            end_time = time.time() + (duration_minutes * 60)
        
        self.logger.info(f"é‡‡é›†é—´éš”: {interval_seconds} ç§’ ({'åˆ†é’Ÿçº§é‡‡é›†' if interval_seconds == 60 else 'è‡ªå®šä¹‰é—´éš”'})")
        
        self.stats["start_time"] = datetime.now().isoformat()
        start_time = time.time()
        batch_number = 1
        last_date = None
        
        try:
            while time.time() < end_time:
                batch_start = time.time()
                current_date = datetime.now().strftime("%Y-%m-%d")
                
                # æ£€æµ‹æ—¥æœŸå˜åŒ–
                if last_date and last_date != current_date:
                    self.logger.info(f"ğŸ—“ï¸ æ—¥æœŸå˜æ›´: {last_date} -> {current_date}")
                    self.logger.info(f"æ–°çš„æ•°æ®å°†ä¿å­˜åˆ° {current_date} æ–‡ä»¶å¤¹")
                
                last_date = current_date
                
                self.logger.info(f"å¼€å§‹ç¬¬ {batch_number} æ‰¹æ¬¡é‡‡é›† ({current_date})...")
                
                # é‡‡é›†æ•°æ®
                all_batch_data = []
                for service in services:
                    traces = self.collect_traces(service=service, lookback="5m", limit=50)
                    if traces:
                        parsed_data = self.parse_traces(traces)
                        all_batch_data.extend(parsed_data)
                    time.sleep(0.5)
                
                # ä¿å­˜æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†è·¨æ—¥æœŸï¼‰
                if all_batch_data:
                    current_time = datetime.now().isoformat()
                    self.save_data(all_batch_data, current_time)
                    self.collected_data.extend(all_batch_data)
                
                # æ˜¾ç¤ºè¿›åº¦
                if duration_minutes > 0:
                    elapsed_minutes = (time.time() - start_time) / 60
                    progress_info = f"è¿›åº¦: {elapsed_minutes:.1f}/{duration_minutes}åˆ†é’Ÿ"
                else:
                    elapsed_hours = (time.time() - start_time) / 3600
                    progress_info = f"å·²è¿è¡Œ: {elapsed_hours:.1f}å°æ—¶"
                
                total_spans = self.stats["total_spans"]
                parent_rate = (self.stats["spans_with_parent"] / max(total_spans, 1)) * 100
                db_rate = (self.stats["db_spans"] / max(total_spans, 1)) * 100
                
                self.logger.info(f"{progress_info} | "
                               f"Spanæ€»æ•°: {total_spans} | "
                               f"çˆ¶å­å…³ç³»: {parent_rate:.1f}% | "
                               f"DBå“ˆå¸Œ: {db_rate:.1f}%")
                
                batch_number += 1
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªé‡‡é›†å‘¨æœŸ
                batch_duration = time.time() - batch_start
                sleep_time = max(0, interval_seconds - batch_duration)
                if sleep_time > 0:
                    time.sleep(sleep_time)
        
        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­é‡‡é›†")
        except Exception as e:
            self.logger.error(f"é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.stats["end_time"] = datetime.now().isoformat()
            self._print_final_stats()
        
        self.logger.info("æ•°æ®é‡‡é›†å®Œæˆ")
        return True

    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        total_spans = len(self.collected_data)
        if total_spans == 0:
            self.logger.warning("æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®")
            return
        
        error_spans = len([s for s in self.collected_data if s.get("_has_error", False)])
        parent_spans = len([s for s in self.collected_data if s.get("parentSpanId", 0) > 0])
        db_hash_spans = len([s for s in self.collected_data if s.get("DBhash", 0) > 0])
        
        self.logger.info("=== é‡‡é›†ç»Ÿè®¡ä¿¡æ¯ ===")
        self.logger.info(f"Span æ€»æ•°: {total_spans:,}")
        self.logger.info(f"é”™è¯¯ Span: {error_spans:,} ({(error_spans/total_spans)*100:.1f}%)")
        self.logger.info(f"æœ‰çˆ¶å­å…³ç³»çš„ Span: {parent_spans:,} ({(parent_spans/total_spans)*100:.1f}%)")
        self.logger.info(f"æœ‰ DB å“ˆå¸Œçš„ Span: {db_hash_spans:,} ({(db_hash_spans/total_spans)*100:.1f}%)")
        self.logger.info(f"å”¯ä¸€æœåŠ¡æ•°: {len(self.service_encoder)}")
        self.logger.info(f"å”¯ä¸€æ“ä½œæ•°: {len(self.operation_encoder)}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Train Ticket é“¾è·¯è¿½è¸ªæ•°æ®é‡‡é›†å™¨",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python trace_collector.py --test                    # æµ‹è¯•è¿æ¥
  python trace_collector.py --duration 60            # é‡‡é›†1å°æ—¶
  python trace_collector.py --duration 0             # æŒç»­è¿è¡Œ
  python trace_collector.py --duration 1440 --interval 60  # é‡‡é›†24å°æ—¶ï¼Œé—´éš”1åˆ†é’Ÿ
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--duration", type=int, default=30, 
                       help="é‡‡é›†æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œ0=æŒç»­è¿è¡Œï¼Œé»˜è®¤: 30")
    parser.add_argument("--interval", type=int, default=None, 
                       help=f"é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: {Config().DEFAULT_COLLECTION_INTERVAL}")
    parser.add_argument("--test", action="store_true", 
                       help="ä»…æµ‹è¯•è¿æ¥")
    
    args = parser.parse_args()
    
    collector = AnomalyDetectionTraceCollector()
    
    if args.test:
        print("æ­£åœ¨æµ‹è¯•è¿æ¥...")
        return 0 if collector.test_connection() else 1
    
    try:
        success = collector.start_collection(
            duration_minutes=args.duration,
            interval_seconds=args.interval
        )
        return 0 if success else 1
    except KeyboardInterrupt:
        print("é‡‡é›†å·²ä¸­æ–­")
        return 0

if __name__ == "__main__":
    exit(main())